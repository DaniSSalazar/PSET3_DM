{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "baa46352-7552-4e39-959a-866b544c55fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"IngestNYCTaxiData\") \\\n",
    "    .config(\"spark.jars\", \"/home/jovyan/jars/snowflake-jdbc-3.13.30.jar,/home/jovyan/jars/spark-snowflake_2.12-2.12.0-spark_3.4.jar\") \\\n",
    "    .getOrCreate()\n",
    "#en la sesión de Spark se incluye los archivos jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5069b56d-5382-4149-a502-afd39332f6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#obtener credenciales y guardarlas\n",
    "import os\n",
    "\n",
    "sfOptions = {\n",
    "    \"sfURL\": f\"https://{os.getenv('SF_ACCOUNT')}.snowflakecomputing.com\",\n",
    "    \"sfUser\": os.getenv(\"SF_USER\"),\n",
    "    \"sfPassword\": os.getenv(\"SF_PASSWORD\"),\n",
    "    \"sfWarehouse\": os.getenv(\"SF_WAREHOUSE\"),\n",
    "    \"sfDatabase\": os.getenv(\"SF_DATABASE\"),\n",
    "    \"sfSchema\": os.getenv(\"SF_SCHEMA\"),\n",
    "    \"sfRole\": os.getenv(\"SF_ROLE\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61358fbc-6f76-48d7-918a-5d1ffc0bd855",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+------------------+----------------+------------+-----------+------------+--------------------+--------------------+\n",
      "|VENDORID|TPEP_PICKUP_DATETIME|TPEP_DROPOFF_DATETIME|PASSENGER_COUNT|TRIP_DISTANCE|RATECODEID|STORE_AND_FWD_FLAG|PULOCATIONID|DOLOCATIONID|PAYMENT_TYPE|FARE_AMOUNT|EXTRA|MTA_TAX|TIP_AMOUNT|TOLLS_AMOUNT|IMPROVEMENT_SURCHARGE|TOTAL_AMOUNT|CONGESTION_SURCHARGE|AIRPORT_FEE|CBD_CONGESTION_FEE|          RUN_ID|SERVICE_TYPE|SOURCE_YEAR|SOURCE_MONTH|     INGESTED_AT_UTC|         SOURCE_PATH|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+------------------+----------------+------------+-----------+------------+--------------------+--------------------+\n",
      "|       2| 2015-02-10 09:34:11|  2015-02-10 09:40:08|            2.0|         0.86|       1.0|                 N|          79|         249|           2|        6.0|  0.0|    0.5|       0.0|         0.0|                  0.3|         6.8|                NULL|       NULL|              NULL|YEL_2015_02_0004|      YELLOW|       2015|           2|2025-10-17 02:45:...|https://d37ci6vzu...|\n",
      "|       2| 2015-02-10 09:47:32|  2015-02-10 09:54:12|            2.0|         0.79|       1.0|                 N|         125|         144|           2|        6.0|  0.0|    0.5|       0.0|         0.0|                  0.3|         6.8|                NULL|       NULL|              NULL|YEL_2015_02_0004|      YELLOW|       2015|           2|2025-10-17 02:45:...|https://d37ci6vzu...|\n",
      "|       2| 2015-02-10 09:24:50|  2015-02-10 09:35:23|            1.0|         1.15|       1.0|                 N|         113|          68|           1|        8.0|  0.0|    0.5|       2.0|         0.0|                  0.3|        10.8|                NULL|       NULL|              NULL|YEL_2015_02_0004|      YELLOW|       2015|           2|2025-10-17 02:45:...|https://d37ci6vzu...|\n",
      "|       2| 2015-02-10 09:53:54|  2015-02-10 10:22:03|            1.0|         3.42|       1.0|                 N|          68|         231|           2|       18.5|  0.0|    0.5|       0.0|         0.0|                  0.3|        19.3|                NULL|       NULL|              NULL|YEL_2015_02_0004|      YELLOW|       2015|           2|2025-10-17 02:45:...|https://d37ci6vzu...|\n",
      "|       1| 2015-02-10 09:00:35|  2015-02-10 09:05:36|            1.0|          0.9|       1.0|                 N|         140|         263|           1|        5.5|  0.0|    0.5|       1.0|         0.0|                  0.3|         7.3|                NULL|       NULL|              NULL|YEL_2015_02_0004|      YELLOW|       2015|           2|2025-10-17 02:45:...|https://d37ci6vzu...|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+------------------+----------------+------------+-----------+------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#crear un dataframe para leer la tabla de NYC_YELLOW_TAXIS en Snowflake\n",
    "df = spark.read \\\n",
    "    .format(\"snowflake\") \\\n",
    "    .options(**sfOptions) \\\n",
    "    .option(\"dbtable\", \"RAW.NYC_YELLOW_TAXIS\") \\\n",
    "    .load()\n",
    "\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6903566b-80d2-4d84-a3ac-dd36d0bb5328",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================\n",
    "# Ingesta Yellow Taxi → Snowflake por chunks (JDBC puro)\n",
    "# =======================\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import (\n",
    "    col, lit, current_timestamp, row_number, monotonically_increasing_id\n",
    ")\n",
    "from pyspark.sql.window import Window\n",
    "import math\n",
    "\n",
    "# =======================\n",
    "# Configuración\n",
    "# =======================\n",
    "SERVICE_TYPE = \"YELLOW\"\n",
    "CHUNK_SIZE = 1_000_000  # tamaño del chunk\n",
    "\n",
    "# =======================\n",
    "# Columnas destino Snowflake\n",
    "# (RAW.NYC_YELLOW_TAXIS → según DESC TABLE que compartiste)\n",
    "# =======================\n",
    "SNOWFLAKE_FINAL_COLS = [\n",
    "    \"VendorID\", \"tpep_pickup_datetime\", \"tpep_dropoff_datetime\",\n",
    "    \"passenger_count\", \"trip_distance\", \"RatecodeID\", \"store_and_fwd_flag\",\n",
    "    \"PULocationID\", \"DOLocationID\", \"payment_type\", \"fare_amount\", \"extra\",\n",
    "    \"mta_tax\", \"tip_amount\", \"tolls_amount\", \"improvement_surcharge\",\n",
    "    \"total_amount\", \"congestion_surcharge\", \"airport_fee\", \"cbd_congestion_fee\",\n",
    "    \"RUN_ID\", \"SERVICE_TYPE\", \"SOURCE_YEAR\", \"SOURCE_MONTH\",\n",
    "    \"INGESTED_AT_UTC\", \"SOURCE_PATH\"\n",
    "]\n",
    "\n",
    "# Auditoría (RAW.INGEST_AUDIT)\n",
    "AUDIT_COLS_ORDER = [\n",
    "    \"RUN_ID\", \"SERVICE_TYPE\", \"SOURCE_YEAR\", \"SOURCE_MONTH\",\n",
    "    \"INGESTED_AT_UTC\", \"SOURCE_PATH\", \"ROW_COUNT\"\n",
    "]\n",
    "\n",
    "# =======================\n",
    "# Utilidades de esquema\n",
    "# =======================\n",
    "def _ensure_column_present(df: DataFrame, candidates, target, spark_type) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Si existe alguna columna en 'candidates', la renombra a 'target'.\n",
    "    Si no existe, crea 'target' como NULL del tipo 'spark_type'.\n",
    "    \"\"\"\n",
    "    existing = set(df.columns)\n",
    "    for c in candidates:\n",
    "        if c in existing:\n",
    "            if c != target:\n",
    "                return df.withColumnRenamed(c, target)\n",
    "            else:\n",
    "                return df\n",
    "    return df.withColumn(target, lit(None).cast(spark_type))\n",
    "\n",
    "def _align_and_enrich_schema(base_df: DataFrame, run_id: str, year: int, month: int, source_url: str) -> DataFrame:\n",
    "    \"\"\"\n",
    "    - Castea timestamps\n",
    "    - Rellena columnas modernas si faltan (airport_fee, cbd_congestion_fee)\n",
    "    - Agrega metadatos (RUN_ID, SERVICE_TYPE, SOURCE_YEAR, SOURCE_MONTH, INGESTED_AT_UTC, SOURCE_PATH)\n",
    "    - Reordena columnas exactamente como Snowflake las espera\n",
    "    \"\"\"\n",
    "    df = base_df\n",
    "\n",
    "    # Timestamps → timestamp (Spark) → Snowflake TIMESTAMP_NTZ\n",
    "    if \"tpep_pickup_datetime\" in df.columns:\n",
    "        df = df.withColumn(\"tpep_pickup_datetime\", col(\"tpep_pickup_datetime\").cast(\"timestamp\"))\n",
    "    if \"tpep_dropoff_datetime\" in df.columns:\n",
    "        df = df.withColumn(\"tpep_dropoff_datetime\", col(\"tpep_dropoff_datetime\").cast(\"timestamp\"))\n",
    "\n",
    "    # airport_fee en minúscula (en 2015 puede venir como airport_fee o Airport_fee)\n",
    "    df = _ensure_column_present(df, [\"airport_fee\", \"Airport_fee\"], \"airport_fee\", \"float\")\n",
    "\n",
    "    # 2015 NO trae cbd_congestion_fee: crear NULL si no está\n",
    "    df = _ensure_column_present(df, [\"cbd_congestion_fee\", \"CBD_CONGESTION_FEE\"], \"cbd_congestion_fee\", \"float\")\n",
    "\n",
    "    # Metadatos\n",
    "    df = (\n",
    "        df.withColumn(\"RUN_ID\", lit(run_id))\n",
    "          .withColumn(\"SERVICE_TYPE\", lit(SERVICE_TYPE))\n",
    "          .withColumn(\"SOURCE_YEAR\", lit(year))\n",
    "          .withColumn(\"SOURCE_MONTH\", lit(month))\n",
    "          .withColumn(\"INGESTED_AT_UTC\", current_timestamp())\n",
    "          .withColumn(\"SOURCE_PATH\", lit(source_url))\n",
    "    )\n",
    "\n",
    "    # Orden EXACTO como Snowflake\n",
    "    df = df.select(SNOWFLAKE_FINAL_COLS)\n",
    "    return df\n",
    "\n",
    "# =======================\n",
    "# Idempotencia por RUN_ID\n",
    "# =======================\n",
    "def _delete_previous_chunk_sql(run_id: str) -> str:\n",
    "    \"\"\"SQL que limpia datos previos de ese RUN_ID en ambas tablas antes de cargar.\"\"\"\n",
    "    return f\"\"\"\n",
    "        DELETE FROM RAW.NYC_YELLOW_TAXIS WHERE RUN_ID = '{run_id}';\n",
    "        DELETE FROM RAW.INGEST_AUDIT     WHERE RUN_ID = '{run_id}';\n",
    "    \"\"\"\n",
    "\n",
    "# =======================\n",
    "# Writes JDBC puros\n",
    "# =======================\n",
    "def _write_chunk_to_snowflake(df_chunk: DataFrame, sfOptions: dict, run_id: str):\n",
    "    \"\"\"\n",
    "    Escribe el chunk en RAW.NYC_YELLOW_TAXIS forzando JDBC (sin COPY/Stage).\n",
    "    \"\"\"\n",
    "    pre_sql = _delete_previous_chunk_sql(run_id)\n",
    "    (\n",
    "        df_chunk.write\n",
    "        .format(\"snowflake\")\n",
    "        .options(**sfOptions)\n",
    "        .option(\"dbtable\", \"RAW.NYC_YELLOW_TAXIS\")\n",
    "        .option(\"preactions\", pre_sql)\n",
    "        .option(\"usestagetable\", \"false\")     # ← evita staging table\n",
    "        .option(\"use_copy_unload\", \"false\")   # ← fuerza JDBC puro (sin COPY)\n",
    "        .mode(\"append\")\n",
    "        .save()\n",
    "    )\n",
    "\n",
    "def _write_audit_row(sfOptions: dict, run_id: str, row_count: int, spark_session, year: int, month: int, source_url: str):\n",
    "    \"\"\"\n",
    "    Inserta la fila de auditoría del chunk en RAW.INGEST_AUDIT\n",
    "    usando Spark DataFrame (sin pandas).\n",
    "    \"\"\"\n",
    "    audit_df = spark_session.range(1).select(\n",
    "        lit(run_id).alias(\"RUN_ID\"),\n",
    "        lit(SERVICE_TYPE).alias(\"SERVICE_TYPE\"),\n",
    "        lit(year).alias(\"SOURCE_YEAR\"),\n",
    "        lit(month).alias(\"SOURCE_MONTH\"),\n",
    "        current_timestamp().alias(\"INGESTED_AT_UTC\"),\n",
    "        lit(source_url).alias(\"SOURCE_PATH\"),\n",
    "        lit(row_count).cast(\"long\").alias(\"ROW_COUNT\")\n",
    "    )\n",
    "\n",
    "    pre_sql = f\"DELETE FROM RAW.INGEST_AUDIT WHERE RUN_ID = '{run_id}';\"\n",
    "    (\n",
    "        audit_df.write\n",
    "        .format(\"snowflake\")\n",
    "        .options(**sfOptions)\n",
    "        .option(\"dbtable\", \"RAW.INGEST_AUDIT\")\n",
    "        .option(\"preactions\", pre_sql)\n",
    "        .option(\"usestagetable\", \"false\")     # ← evita staging table\n",
    "        .option(\"use_copy_unload\", \"false\")   # ← fuerza JDBC puro (sin COPY)\n",
    "        .mode(\"append\")\n",
    "        .save()\n",
    "    )\n",
    "\n",
    "# =======================\n",
    "# Función principal\n",
    "# =======================\n",
    "def ingest_yellow_month_chunked(\n",
    "    spark,\n",
    "    sfOptions: dict,\n",
    "    file_path: str,\n",
    "    year: int,\n",
    "    month: int,\n",
    "    chunk_size: int = CHUNK_SIZE\n",
    "):\n",
    "    \"\"\"\n",
    "    Lee el Parquet del mes indicado, divide en chunks de 'chunk_size',\n",
    "    alinea esquema dinámicamente, escribe cada chunk con RUN_ID = YEL_YYYY_MM_XXXX,\n",
    "    y registra auditoría por chunk.\n",
    "    \"\"\"\n",
    "    source_url = f\"https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_{year}-{month:02d}.parquet\"\n",
    "\n",
    "    print(f\"📥 Leyendo Parquet local: {file_path}\")\n",
    "    base_df = spark.read.parquet(file_path)\n",
    "\n",
    "    # Indexado estable para chunking\n",
    "    w = Window.orderBy(monotonically_increasing_id())\n",
    "    df_indexed = base_df.withColumn(\"_rn\", row_number().over(w))\n",
    "\n",
    "    total_rows = df_indexed.count()\n",
    "    num_chunks = math.ceil(total_rows / chunk_size)\n",
    "    print(f\"📊 Filas totales: {total_rows} → Chunks de {chunk_size}: {num_chunks}\")\n",
    "\n",
    "    base_run_prefix = f\"YEL_{year}_{month:02d}_\"\n",
    "\n",
    "    for i in range(num_chunks):\n",
    "        start = i * chunk_size + 1\n",
    "        end = min((i + 1) * chunk_size, total_rows)\n",
    "        run_id = f\"{base_run_prefix}{(i+1):04d}\"\n",
    "\n",
    "        print(f\"🧩 Chunk {i+1}/{num_chunks} → filas [{start}, {end}] → RUN_ID={run_id}\")\n",
    "\n",
    "        # Rebanar el chunk\n",
    "        chunk_base = df_indexed.filter((col(\"_rn\") >= start) & (col(\"_rn\") <= end)).drop(\"_rn\")\n",
    "\n",
    "        # Alinear, enriquecer y reordenar columnas al estándar Snowflake\n",
    "        chunk_df_for_sf = _align_and_enrich_schema(chunk_base, run_id, year, month, source_url)\n",
    "\n",
    "        # Escribir chunk a tabla principal (con preactions para idempotencia)\n",
    "        _write_chunk_to_snowflake(chunk_df_for_sf, sfOptions, run_id)\n",
    "\n",
    "        # Registrar auditoría del chunk\n",
    "        _write_audit_row(sfOptions, run_id, row_count=(end - start + 1), spark_session=spark,\n",
    "                         year=year, month=month, source_url=source_url)\n",
    "\n",
    "    print(\"✅ Ingesta mensual chunked COMPLETADA con auditoría por chunk.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "030d5802-7383-4e81-8456-5a51ebfb0e65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📥 Leyendo Parquet local: /home/work/yellow_2015_02.parquet\n",
      "📊 Filas totales: 12442394 → Chunks de 1000000: 13\n",
      "🧩 Chunk 1/13 → filas [1, 1000000] → RUN_ID=YEL_2015_02_0001\n",
      "🧩 Chunk 2/13 → filas [1000001, 2000000] → RUN_ID=YEL_2015_02_0002\n",
      "🧩 Chunk 3/13 → filas [2000001, 3000000] → RUN_ID=YEL_2015_02_0003\n",
      "🧩 Chunk 4/13 → filas [3000001, 4000000] → RUN_ID=YEL_2015_02_0004\n",
      "🧩 Chunk 5/13 → filas [4000001, 5000000] → RUN_ID=YEL_2015_02_0005\n",
      "🧩 Chunk 6/13 → filas [5000001, 6000000] → RUN_ID=YEL_2015_02_0006\n",
      "🧩 Chunk 7/13 → filas [6000001, 7000000] → RUN_ID=YEL_2015_02_0007\n",
      "🧩 Chunk 8/13 → filas [7000001, 8000000] → RUN_ID=YEL_2015_02_0008\n",
      "🧩 Chunk 9/13 → filas [8000001, 9000000] → RUN_ID=YEL_2015_02_0009\n",
      "🧩 Chunk 10/13 → filas [9000001, 10000000] → RUN_ID=YEL_2015_02_0010\n",
      "🧩 Chunk 11/13 → filas [10000001, 11000000] → RUN_ID=YEL_2015_02_0011\n",
      "🧩 Chunk 12/13 → filas [11000001, 12000000] → RUN_ID=YEL_2015_02_0012\n",
      "🧩 Chunk 13/13 → filas [12000001, 12442394] → RUN_ID=YEL_2015_02_0013\n",
      "✅ Ingesta mensual chunked COMPLETADA con auditoría por chunk.\n"
     ]
    }
   ],
   "source": [
    "file_path = \"/home/work/yellow_2015_02.parquet\"\n",
    "\n",
    "ingest_yellow_month_chunked(\n",
    "    spark=spark,\n",
    "    sfOptions=sfOptions,   # ya definido con credenciales\n",
    "    file_path=file_path,\n",
    "    year=2015,\n",
    "    month=2,\n",
    "    chunk_size=1_000_000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b612b1f8-a859-4eee-829f-cf5c5559f2c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Iniciando ingesta mensual: 2017-08 → 2017-09\n",
      "\n",
      "================================================================================\n",
      "📅 Procesando 2017-08 → /home/work/yellow_2017_08.parquet\n",
      "📥 Leyendo Parquet local: /home/work/yellow_2017_08.parquet\n",
      "❌ Error en 2017-08: An error occurred while calling o50.parquet.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1) (5f892c4bd431 executor driver): org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.util.ThreadUtils$.parmap(ThreadUtils.scala:383)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.readParquetFootersInParallel(ParquetFileFormat.scala:443)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1(ParquetFileFormat.scala:493)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1$adapted(ParquetFileFormat.scala:485)\n",
      "\tat org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$2(SchemaMergeUtils.scala:79)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "Caused by: org.apache.spark.SparkException: [CANNOT_READ_FILE_FOOTER] Could not read footer for file: file:/home/work/yellow_2017_08.parquet. Please ensure that the file is in either ORC or Parquet format. If not, please convert it to a valid format. If the file is in the valid format, please check if it is corrupt. If it is, you can choose to either ignore it or fix the corruption.\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.cannotReadFooterForFileError(QueryExecutionErrors.scala:1056)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:456)\n",
      "\tat org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:380)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1395)\n",
      "\tat java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)\n",
      "\tat java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)\n",
      "\tat java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)\n",
      "\tat java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)\n",
      "\tat java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)\n",
      "Caused by: java.lang.RuntimeException: file:/home/work/yellow_2017_08.parquet is not a Parquet file. Expected magic number at tail, but found [55, 85, -30, -105]\n",
      "\tat org.apache.parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:565)\n",
      "\tat org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:799)\n",
      "\tat org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:666)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:85)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:76)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:450)\n",
      "\t... 14 more\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1046)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:407)\n",
      "\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1045)\n",
      "\tat org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.mergeSchemasInParallel(SchemaMergeUtils.scala:73)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.mergeSchemasInParallel(ParquetFileFormat.scala:497)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetUtils$.inferSchema(ParquetUtils.scala:132)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.inferSchema(ParquetFileFormat.scala:79)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource.$anonfun$getOrInferFileFormatSchema$11(DataSource.scala:208)\n",
      "\tat scala.Option.orElse(Option.scala:447)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:205)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:407)\n",
      "\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)\n",
      "\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n",
      "\tat org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:563)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.util.ThreadUtils$.parmap(ThreadUtils.scala:383)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.readParquetFootersInParallel(ParquetFileFormat.scala:443)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1(ParquetFileFormat.scala:493)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1$adapted(ParquetFileFormat.scala:485)\n",
      "\tat org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$2(SchemaMergeUtils.scala:79)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\t... 1 more\n",
      "Caused by: org.apache.spark.SparkException: [CANNOT_READ_FILE_FOOTER] Could not read footer for file: file:/home/work/yellow_2017_08.parquet. Please ensure that the file is in either ORC or Parquet format. If not, please convert it to a valid format. If the file is in the valid format, please check if it is corrupt. If it is, you can choose to either ignore it or fix the corruption.\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.cannotReadFooterForFileError(QueryExecutionErrors.scala:1056)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:456)\n",
      "\tat org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:380)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1395)\n",
      "\tat java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)\n",
      "\tat java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)\n",
      "\tat java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)\n",
      "\tat java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)\n",
      "\tat java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)\n",
      "Caused by: java.lang.RuntimeException: file:/home/work/yellow_2017_08.parquet is not a Parquet file. Expected magic number at tail, but found [55, 85, -30, -105]\n",
      "\tat org.apache.parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:565)\n",
      "\tat org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:799)\n",
      "\tat org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:666)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:85)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:76)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:450)\n",
      "\t... 14 more\n",
      "\n",
      "\n",
      "================================================================================\n",
      "📅 Procesando 2017-09 → /home/work/yellow_2017_09.parquet\n",
      "📥 Leyendo Parquet local: /home/work/yellow_2017_09.parquet\n",
      "❌ Error en 2017-09: An error occurred while calling o57.parquet.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 1 times, most recent failure: Lost task 0.0 in stage 2.0 (TID 2) (5f892c4bd431 executor driver): org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.util.ThreadUtils$.parmap(ThreadUtils.scala:383)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.readParquetFootersInParallel(ParquetFileFormat.scala:443)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1(ParquetFileFormat.scala:493)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1$adapted(ParquetFileFormat.scala:485)\n",
      "\tat org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$2(SchemaMergeUtils.scala:79)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "Caused by: org.apache.spark.SparkException: [CANNOT_READ_FILE_FOOTER] Could not read footer for file: file:/home/work/yellow_2017_09.parquet. Please ensure that the file is in either ORC or Parquet format. If not, please convert it to a valid format. If the file is in the valid format, please check if it is corrupt. If it is, you can choose to either ignore it or fix the corruption.\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.cannotReadFooterForFileError(QueryExecutionErrors.scala:1056)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:456)\n",
      "\tat org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:380)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1395)\n",
      "\tat java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)\n",
      "\tat java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)\n",
      "\tat java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)\n",
      "\tat java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)\n",
      "\tat java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)\n",
      "Caused by: java.lang.RuntimeException: file:/home/work/yellow_2017_09.parquet is not a Parquet file. Expected magic number at tail, but found [23, 15, 43, 93]\n",
      "\tat org.apache.parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:565)\n",
      "\tat org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:799)\n",
      "\tat org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:666)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:85)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:76)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:450)\n",
      "\t... 14 more\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1046)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:407)\n",
      "\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1045)\n",
      "\tat org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.mergeSchemasInParallel(SchemaMergeUtils.scala:73)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.mergeSchemasInParallel(ParquetFileFormat.scala:497)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetUtils$.inferSchema(ParquetUtils.scala:132)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.inferSchema(ParquetFileFormat.scala:79)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource.$anonfun$getOrInferFileFormatSchema$11(DataSource.scala:208)\n",
      "\tat scala.Option.orElse(Option.scala:447)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:205)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:407)\n",
      "\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)\n",
      "\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n",
      "\tat org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:563)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.util.ThreadUtils$.parmap(ThreadUtils.scala:383)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.readParquetFootersInParallel(ParquetFileFormat.scala:443)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1(ParquetFileFormat.scala:493)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1$adapted(ParquetFileFormat.scala:485)\n",
      "\tat org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$2(SchemaMergeUtils.scala:79)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\t... 1 more\n",
      "Caused by: org.apache.spark.SparkException: [CANNOT_READ_FILE_FOOTER] Could not read footer for file: file:/home/work/yellow_2017_09.parquet. Please ensure that the file is in either ORC or Parquet format. If not, please convert it to a valid format. If the file is in the valid format, please check if it is corrupt. If it is, you can choose to either ignore it or fix the corruption.\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.cannotReadFooterForFileError(QueryExecutionErrors.scala:1056)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:456)\n",
      "\tat org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:380)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1395)\n",
      "\tat java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)\n",
      "\tat java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)\n",
      "\tat java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)\n",
      "\tat java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)\n",
      "\tat java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)\n",
      "Caused by: java.lang.RuntimeException: file:/home/work/yellow_2017_09.parquet is not a Parquet file. Expected magic number at tail, but found [23, 15, 43, 93]\n",
      "\tat org.apache.parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:565)\n",
      "\tat org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:799)\n",
      "\tat org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:666)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:85)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:76)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:450)\n",
      "\t... 14 more\n",
      "\n",
      "\n",
      "================================================================================\n",
      "🏁 Resumen:\n",
      "   ✅ Meses OK     : 0\n",
      "   ⏭  Meses omitidos (sin archivo): 0\n",
      "   ❌ Meses con error             : 2\n",
      "🎉 Terminado.\n"
     ]
    }
   ],
   "source": [
    "# =======================\n",
    "# Runner multi-mes: 2015-03 → 2025-06\n",
    "# =======================\n",
    "import os\n",
    "from datetime import date\n",
    "\n",
    "def month_iter(start_year: int, start_month: int, end_year: int, end_month: int):\n",
    "    \"\"\"Itera (year, month) de inicio a fin, ambos inclusive.\"\"\"\n",
    "    y, m = start_year, start_month\n",
    "    while (y < end_year) or (y == end_year and m <= end_month):\n",
    "        yield y, m\n",
    "        # avanzar un mes\n",
    "        if m == 12:\n",
    "            y, m = y + 1, 1\n",
    "        else:\n",
    "            m += 1\n",
    "\n",
    "# Rango solicitado\n",
    "START_YEAR, START_MONTH = 2015, 1\n",
    "END_YEAR, END_MONTH = 2025, 8\n",
    "\n",
    "base_dir = \"/home/work\"  # carpeta donde están los parquet locales\n",
    "ok, skipped, failed = 0, 0, 0\n",
    "\n",
    "print(f\"🚀 Iniciando ingesta mensual: {START_YEAR}-{START_MONTH:02d} → {END_YEAR}-{END_MONTH:02d}\")\n",
    "for yy, mm in month_iter(START_YEAR, START_MONTH, END_YEAR, END_MONTH):\n",
    "    file_path = os.path.join(base_dir, f\"yellow_{yy}_{mm:02d}.parquet\")\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"📅 Procesando {yy}-{mm:02d} → {file_path}\")\n",
    "\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"⚠️  Archivo NO encontrado, se omite: {file_path}\")\n",
    "        skipped += 1\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        ingest_yellow_month_chunked(\n",
    "            spark=spark,\n",
    "            sfOptions=sfOptions,  # Debe estar definido en tu notebook\n",
    "            file_path=file_path,\n",
    "            year=yy,\n",
    "            month=mm,\n",
    "            chunk_size=1_000_000\n",
    "        )\n",
    "        ok += 1\n",
    "    except Exception as e:\n",
    "        failed += 1\n",
    "        print(f\"❌ Error en {yy}-{mm:02d}: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"🏁 Resumen:\")\n",
    "print(f\"   ✅ Meses OK     : {ok}\")\n",
    "print(f\"   ⏭  Meses omitidos (sin archivo): {skipped}\")\n",
    "print(f\"   ❌ Meses con error             : {failed}\")\n",
    "print(\"🎉 Terminado.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86f2670b-03a7-46e5-814f-08020da59eeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔎 Muestra de RAW.NYC_GREEN_TAXIS:\n",
      "+--------+--------------------+---------------------+------------------+----------+------------+------------+---------------+-------------+-----------+-----+-------+----------+------------+---------+---------------------+------------+------------+---------+--------------------+------------------+------+------------+-----------+------------+---------------+-----------+\n",
      "|VENDORID|LPEP_PICKUP_DATETIME|LPEP_DROPOFF_DATETIME|STORE_AND_FWD_FLAG|RATECODEID|PULOCATIONID|DOLOCATIONID|PASSENGER_COUNT|TRIP_DISTANCE|FARE_AMOUNT|EXTRA|MTA_TAX|TIP_AMOUNT|TOLLS_AMOUNT|EHAIL_FEE|IMPROVEMENT_SURCHARGE|TOTAL_AMOUNT|PAYMENT_TYPE|TRIP_TYPE|CONGESTION_SURCHARGE|CBD_CONGESTION_FEE|RUN_ID|SERVICE_TYPE|SOURCE_YEAR|SOURCE_MONTH|INGESTED_AT_UTC|SOURCE_PATH|\n",
      "+--------+--------------------+---------------------+------------------+----------+------------+------------+---------------+-------------+-----------+-----+-------+----------+------------+---------+---------------------+------------+------------+---------+--------------------+------------------+------+------------+-----------+------------+---------------+-----------+\n",
      "+--------+--------------------+---------------------+------------------+----------+------------+------------+---------------+-------------+-----------+-----+-------+----------+------------+---------+---------------------+------------+------------+---------+--------------------+------------------+------+------------+-----------+------------+---------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ======================================================================================\n",
    "# Ingesta GREEN Taxi → Snowflake por chunks (JDBC puro) [misma arquitectura que Yellow]\n",
    "# ======================================================================================\n",
    "\n",
    "# -----------------------\n",
    "# Configuración específica\n",
    "# -----------------------\n",
    "SERVICE_TYPE_GREEN = \"GREEN\"\n",
    "CHUNK_SIZE_GREEN   = 500_000  # puedes cambiarlo si quieres\n",
    "\n",
    "# -----------------------\n",
    "# Columnas destino Snowflake (RAW.NYC_GREEN_TAXIS) en el orden exacto que compartiste\n",
    "# -----------------------\n",
    "SNOWFLAKE_FINAL_COLS_GREEN = [\n",
    "    \"VendorID\",\n",
    "    \"lpep_pickup_datetime\", \"lpep_dropoff_datetime\",\n",
    "    \"store_and_fwd_flag\",\n",
    "    \"RatecodeID\",\n",
    "    \"PULocationID\", \"DOLocationID\",\n",
    "    \"passenger_count\",\n",
    "    \"trip_distance\",\n",
    "    \"fare_amount\", \"extra\", \"mta_tax\", \"tip_amount\", \"tolls_amount\",\n",
    "    \"ehail_fee\",\n",
    "    \"improvement_surcharge\",\n",
    "    \"total_amount\",\n",
    "    \"payment_type\",\n",
    "    \"trip_type\",\n",
    "    \"congestion_surcharge\",\n",
    "    \"cbd_congestion_fee\",\n",
    "    # metadatos\n",
    "    \"RUN_ID\", \"SERVICE_TYPE\", \"SOURCE_YEAR\", \"SOURCE_MONTH\",\n",
    "    \"INGESTED_AT_UTC\", \"SOURCE_PATH\"\n",
    "]\n",
    "\n",
    "# -----------------------\n",
    "# Auditoría (RAW.INGEST_AUDIT) – mismo orden que usaste en Yellow\n",
    "# -----------------------\n",
    "AUDIT_COLS_ORDER_GREEN = [\n",
    "    \"RUN_ID\", \"SERVICE_TYPE\", \"SOURCE_YEAR\", \"SOURCE_MONTH\",\n",
    "    \"INGESTED_AT_UTC\", \"SOURCE_PATH\", \"ROW_COUNT\"\n",
    "]\n",
    "\n",
    "# -----------------------\n",
    "# Utilidad: asegurar/renombrar columna (reusa tu _ensure_column_present si ya existe)\n",
    "# -----------------------\n",
    "try:\n",
    "    _ensure_column_present\n",
    "except NameError:\n",
    "    from pyspark.sql.functions import lit\n",
    "    from pyspark.sql import DataFrame\n",
    "    def _ensure_column_present(df: DataFrame, candidates, target, spark_type) -> DataFrame:\n",
    "        existing = set(df.columns)\n",
    "        for c in candidates:\n",
    "            if c in existing:\n",
    "                return df if c == target else df.withColumnRenamed(c, target)\n",
    "        return df.withColumn(target, lit(None).cast(spark_type))\n",
    "\n",
    "# -----------------------\n",
    "# Alinear esquema para GREEN\n",
    "# -----------------------\n",
    "from pyspark.sql.functions import col, current_timestamp, lit\n",
    "\n",
    "def _align_and_enrich_schema_green(base_df, run_id: str, year: int, month: int, source_url: str):\n",
    "    df = base_df\n",
    "\n",
    "    # lpep_* timestamps → Spark timestamp (Snowflake TIMESTAMP_NTZ)\n",
    "    if \"lpep_pickup_datetime\" in df.columns:\n",
    "        df = df.withColumn(\"lpep_pickup_datetime\", col(\"lpep_pickup_datetime\").cast(\"timestamp\"))\n",
    "    if \"lpep_dropoff_datetime\" in df.columns:\n",
    "        df = df.withColumn(\"lpep_dropoff_datetime\", col(\"lpep_dropoff_datetime\").cast(\"timestamp\"))\n",
    "\n",
    "    # Columnas que pueden faltar en ciertos años/meses → crear como NULL si no existen\n",
    "    df = _ensure_column_present(df, [\"ehail_fee\", \"Ehail_fee\", \"E HAIL FEE\"], \"ehail_fee\", \"float\")\n",
    "    df = _ensure_column_present(df, [\"trip_type\", \"Trip_type\"], \"trip_type\", \"float\")\n",
    "    df = _ensure_column_present(df, [\"congestion_surcharge\", \"Congestion_Surcharge\"], \"congestion_surcharge\", \"float\")\n",
    "    df = _ensure_column_present(df, [\"cbd_congestion_fee\", \"CBD_CONGESTION_FEE\"], \"cbd_congestion_fee\", \"float\")\n",
    "\n",
    "    # Metadatos\n",
    "    df = (\n",
    "        df.withColumn(\"RUN_ID\", lit(run_id))\n",
    "          .withColumn(\"SERVICE_TYPE\", lit(SERVICE_TYPE_GREEN))\n",
    "          .withColumn(\"SOURCE_YEAR\", lit(year))\n",
    "          .withColumn(\"SOURCE_MONTH\", lit(month))\n",
    "          .withColumn(\"INGESTED_AT_UTC\", current_timestamp())\n",
    "          .withColumn(\"SOURCE_PATH\", lit(source_url))\n",
    "    )\n",
    "\n",
    "    # Orden EXACTO\n",
    "    df = df.select(SNOWFLAKE_FINAL_COLS_GREEN)\n",
    "    return df\n",
    "\n",
    "# -----------------------\n",
    "# SQL de limpieza idempotente para GREEN\n",
    "# -----------------------\n",
    "def _delete_previous_chunk_sql_green(run_id: str) -> str:\n",
    "    return f\"\"\"\n",
    "        DELETE FROM RAW.NYC_GREEN_TAXIS WHERE RUN_ID = '{run_id}';\n",
    "        DELETE FROM RAW.INGEST_AUDIT     WHERE RUN_ID = '{run_id}';\n",
    "    \"\"\"\n",
    "\n",
    "# -----------------------\n",
    "# Writes JDBC puros (sin COPY/Stage) para GREEN\n",
    "# -----------------------\n",
    "def _write_chunk_to_snowflake_green(df_chunk, sfOptions: dict, run_id: str):\n",
    "    pre_sql = _delete_previous_chunk_sql_green(run_id)\n",
    "    (\n",
    "        df_chunk.write\n",
    "        .format(\"snowflake\")\n",
    "        .options(**sfOptions)\n",
    "        .option(\"dbtable\", \"RAW.NYC_GREEN_TAXIS\")\n",
    "        .option(\"preactions\", pre_sql)\n",
    "        .option(\"usestagetable\", \"false\")\n",
    "        .option(\"use_copy_unload\", \"false\")\n",
    "        .mode(\"append\")\n",
    "        .save()\n",
    "    )\n",
    "\n",
    "def _write_audit_row_green(sfOptions: dict, run_id: str, row_count: int, spark_session, year: int, month: int, source_url: str):\n",
    "    audit_df = spark_session.range(1).select(\n",
    "        lit(run_id).alias(\"RUN_ID\"),\n",
    "        lit(SERVICE_TYPE_GREEN).alias(\"SERVICE_TYPE\"),\n",
    "        lit(year).alias(\"SOURCE_YEAR\"),\n",
    "        lit(month).alias(\"SOURCE_MONTH\"),\n",
    "        current_timestamp().alias(\"INGESTED_AT_UTC\"),\n",
    "        lit(source_url).alias(\"SOURCE_PATH\"),\n",
    "        lit(row_count).cast(\"long\").alias(\"ROW_COUNT\")\n",
    "    )\n",
    "\n",
    "    pre_sql = f\"DELETE FROM RAW.INGEST_AUDIT WHERE RUN_ID = '{run_id}';\"\n",
    "    (\n",
    "        audit_df.write\n",
    "        .format(\"snowflake\")\n",
    "        .options(**sfOptions)\n",
    "        .option(\"dbtable\", \"RAW.INGEST_AUDIT\")\n",
    "        .option(\"preactions\", pre_sql)\n",
    "        .option(\"usestagetable\", \"false\")\n",
    "        .option(\"use_copy_unload\", \"false\")\n",
    "        .mode(\"append\")\n",
    "        .save()\n",
    "    )\n",
    "\n",
    "# -----------------------\n",
    "# Función principal GREEN\n",
    "# -----------------------\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number, monotonically_increasing_id\n",
    "import math\n",
    "\n",
    "def ingest_green_month_chunked(\n",
    "    spark,\n",
    "    sfOptions: dict,\n",
    "    file_path: str,\n",
    "    year: int,\n",
    "    month: int,\n",
    "    chunk_size: int = CHUNK_SIZE_GREEN\n",
    "):\n",
    "    \"\"\"\n",
    "    Lee el Parquet local del mes, divide en chunks, alinea esquema para GREEN,\n",
    "    escribe cada chunk en RAW.NYC_GREEN_TAXIS con RUN_ID = GRE_YYYY_MM_XXXX\n",
    "    y registra auditoría por chunk en RAW.INGEST_AUDIT.\n",
    "    \"\"\"\n",
    "    source_url = f\"https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_{year}-{month:02d}.parquet\"\n",
    "\n",
    "    print(f\"📥 [GREEN] Leyendo Parquet local: {file_path}\")\n",
    "    base_df = spark.read.parquet(file_path)\n",
    "\n",
    "    w = Window.orderBy(monotonically_increasing_id())\n",
    "    df_indexed = base_df.withColumn(\"_rn\", row_number().over(w))\n",
    "\n",
    "    total_rows = df_indexed.count()\n",
    "    num_chunks  = math.ceil(total_rows / chunk_size)\n",
    "    print(f\"📊 [GREEN] Filas totales: {total_rows} → Chunks de {chunk_size}: {num_chunks}\")\n",
    "\n",
    "    base_run_prefix = f\"GRE_{year}_{month:02d}_\"\n",
    "\n",
    "    for i in range(num_chunks):\n",
    "        start = i * chunk_size + 1\n",
    "        end   = min((i + 1) * chunk_size, total_rows)\n",
    "        run_id = f\"{base_run_prefix}{(i+1):04d}\"\n",
    "\n",
    "        print(f\"🧩 [GREEN] Chunk {i+1}/{num_chunks} → filas [{start}, {end}] → RUN_ID={run_id}\")\n",
    "\n",
    "        # Slicing\n",
    "        chunk_base = df_indexed.filter((col(\"_rn\") >= start) & (col(\"_rn\") <= end)).drop(\"_rn\")\n",
    "\n",
    "        # Alinear y enriquecer\n",
    "        chunk_df_for_sf = _align_and_enrich_schema_green(chunk_base, run_id, year, month, source_url)\n",
    "\n",
    "        # Escribir datos y auditoría\n",
    "        _write_chunk_to_snowflake_green(chunk_df_for_sf, sfOptions, run_id)\n",
    "        _write_audit_row_green(sfOptions, run_id, row_count=(end - start + 1),\n",
    "                               spark_session=spark, year=year, month=month, source_url=source_url)\n",
    "\n",
    "    print(\"✅ [GREEN] Ingesta mensual chunked COMPLETADA con auditoría por chunk.\")\n",
    "# -----------------------\n",
    "# Prueba rápida de lectura (opcional)\n",
    "# -----------------------\n",
    "try:\n",
    "    df_green = (\n",
    "        spark.read\n",
    "        .format(\"snowflake\")\n",
    "        .options(**sfOptions)\n",
    "        .option(\"dbtable\", \"RAW.NYC_GREEN_TAXIS\")\n",
    "        .load()\n",
    "    )\n",
    "    print(\"🔎 Muestra de RAW.NYC_GREEN_TAXIS:\")\n",
    "    df_green.show(5)\n",
    "except Exception as e:\n",
    "    print(f\"⚠️ No se pudo leer RAW.NYC_GREEN_TAXIS todavía (quizá no existe o no hay permisos): {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d04de5e-78f7-4f3b-9ee7-020ac54a0352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 [GREEN] Iniciando ingesta mensual: 2016-02 → 2025-08\n",
      "\n",
      "================================================================================\n",
      "📅 [GREEN] Procesando 2016-02 → /home/work/green_2016_02.parquet\n",
      "📥 [GREEN] Leyendo Parquet local: /home/work/green_2016_02.parquet\n",
      "📊 [GREEN] Filas totales: 1510722 → Chunks de 500000: 4\n",
      "🧩 [GREEN] Chunk 1/4 → filas [1, 500000] → RUN_ID=GRE_2016_02_0001\n",
      "🧩 [GREEN] Chunk 2/4 → filas [500001, 1000000] → RUN_ID=GRE_2016_02_0002\n",
      "🧩 [GREEN] Chunk 3/4 → filas [1000001, 1500000] → RUN_ID=GRE_2016_02_0003\n",
      "🧩 [GREEN] Chunk 4/4 → filas [1500001, 1510722] → RUN_ID=GRE_2016_02_0004\n",
      "✅ [GREEN] Ingesta mensual chunked COMPLETADA con auditoría por chunk.\n",
      "\n",
      "================================================================================\n",
      "📅 [GREEN] Procesando 2016-03 → /home/work/green_2016_03.parquet\n",
      "📥 [GREEN] Leyendo Parquet local: /home/work/green_2016_03.parquet\n",
      "📊 [GREEN] Filas totales: 1576393 → Chunks de 500000: 4\n",
      "🧩 [GREEN] Chunk 1/4 → filas [1, 500000] → RUN_ID=GRE_2016_03_0001\n",
      "🧩 [GREEN] Chunk 2/4 → filas [500001, 1000000] → RUN_ID=GRE_2016_03_0002\n",
      "🧩 [GREEN] Chunk 3/4 → filas [1000001, 1500000] → RUN_ID=GRE_2016_03_0003\n",
      "🧩 [GREEN] Chunk 4/4 → filas [1500001, 1576393] → RUN_ID=GRE_2016_03_0004\n",
      "✅ [GREEN] Ingesta mensual chunked COMPLETADA con auditoría por chunk.\n",
      "\n",
      "================================================================================\n",
      "📅 [GREEN] Procesando 2016-04 → /home/work/green_2016_04.parquet\n",
      "📥 [GREEN] Leyendo Parquet local: /home/work/green_2016_04.parquet\n",
      "📊 [GREEN] Filas totales: 1543926 → Chunks de 500000: 4\n",
      "🧩 [GREEN] Chunk 1/4 → filas [1, 500000] → RUN_ID=GRE_2016_04_0001\n",
      "🧩 [GREEN] Chunk 2/4 → filas [500001, 1000000] → RUN_ID=GRE_2016_04_0002\n",
      "🧩 [GREEN] Chunk 3/4 → filas [1000001, 1500000] → RUN_ID=GRE_2016_04_0003\n",
      "🧩 [GREEN] Chunk 4/4 → filas [1500001, 1543926] → RUN_ID=GRE_2016_04_0004\n",
      "✅ [GREEN] Ingesta mensual chunked COMPLETADA con auditoría por chunk.\n",
      "\n",
      "================================================================================\n",
      "📅 [GREEN] Procesando 2016-05 → /home/work/green_2016_05.parquet\n",
      "📥 [GREEN] Leyendo Parquet local: /home/work/green_2016_05.parquet\n",
      "📊 [GREEN] Filas totales: 1536979 → Chunks de 500000: 4\n",
      "🧩 [GREEN] Chunk 1/4 → filas [1, 500000] → RUN_ID=GRE_2016_05_0001\n",
      "🧩 [GREEN] Chunk 2/4 → filas [500001, 1000000] → RUN_ID=GRE_2016_05_0002\n",
      "🧩 [GREEN] Chunk 3/4 → filas [1000001, 1500000] → RUN_ID=GRE_2016_05_0003\n",
      "🧩 [GREEN] Chunk 4/4 → filas [1500001, 1536979] → RUN_ID=GRE_2016_05_0004\n",
      "✅ [GREEN] Ingesta mensual chunked COMPLETADA con auditoría por chunk.\n",
      "\n",
      "================================================================================\n",
      "📅 [GREEN] Procesando 2016-06 → /home/work/green_2016_06.parquet\n",
      "📥 [GREEN] Leyendo Parquet local: /home/work/green_2016_06.parquet\n",
      "📊 [GREEN] Filas totales: 1404727 → Chunks de 500000: 3\n",
      "🧩 [GREEN] Chunk 1/3 → filas [1, 500000] → RUN_ID=GRE_2016_06_0001\n",
      "🧩 [GREEN] Chunk 2/3 → filas [500001, 1000000] → RUN_ID=GRE_2016_06_0002\n",
      "🧩 [GREEN] Chunk 3/3 → filas [1000001, 1404727] → RUN_ID=GRE_2016_06_0003\n",
      "✅ [GREEN] Ingesta mensual chunked COMPLETADA con auditoría por chunk.\n",
      "\n",
      "================================================================================\n",
      "📅 [GREEN] Procesando 2016-07 → /home/work/green_2016_07.parquet\n",
      "📥 [GREEN] Leyendo Parquet local: /home/work/green_2016_07.parquet\n",
      "📊 [GREEN] Filas totales: 1332510 → Chunks de 500000: 3\n",
      "🧩 [GREEN] Chunk 1/3 → filas [1, 500000] → RUN_ID=GRE_2016_07_0001\n",
      "🧩 [GREEN] Chunk 2/3 → filas [500001, 1000000] → RUN_ID=GRE_2016_07_0002\n",
      "🧩 [GREEN] Chunk 3/3 → filas [1000001, 1332510] → RUN_ID=GRE_2016_07_0003\n",
      "✅ [GREEN] Ingesta mensual chunked COMPLETADA con auditoría por chunk.\n",
      "\n",
      "================================================================================\n",
      "📅 [GREEN] Procesando 2016-08 → /home/work/green_2016_08.parquet\n",
      "📥 [GREEN] Leyendo Parquet local: /home/work/green_2016_08.parquet\n",
      "📊 [GREEN] Filas totales: 1247675 → Chunks de 500000: 3\n",
      "🧩 [GREEN] Chunk 1/3 → filas [1, 500000] → RUN_ID=GRE_2016_08_0001\n",
      "🧩 [GREEN] Chunk 2/3 → filas [500001, 1000000] → RUN_ID=GRE_2016_08_0002\n",
      "🧩 [GREEN] Chunk 3/3 → filas [1000001, 1247675] → RUN_ID=GRE_2016_08_0003\n",
      "✅ [GREEN] Ingesta mensual chunked COMPLETADA con auditoría por chunk.\n",
      "\n",
      "================================================================================\n",
      "📅 [GREEN] Procesando 2016-09 → /home/work/green_2016_09.parquet\n",
      "📥 [GREEN] Leyendo Parquet local: /home/work/green_2016_09.parquet\n",
      "📊 [GREEN] Filas totales: 1162373 → Chunks de 500000: 3\n",
      "🧩 [GREEN] Chunk 1/3 → filas [1, 500000] → RUN_ID=GRE_2016_09_0001\n",
      "🧩 [GREEN] Chunk 2/3 → filas [500001, 1000000] → RUN_ID=GRE_2016_09_0002\n",
      "🧩 [GREEN] Chunk 3/3 → filas [1000001, 1162373] → RUN_ID=GRE_2016_09_0003\n",
      "✅ [GREEN] Ingesta mensual chunked COMPLETADA con auditoría por chunk.\n",
      "\n",
      "================================================================================\n",
      "📅 [GREEN] Procesando 2016-10 → /home/work/green_2016_10.parquet\n",
      "📥 [GREEN] Leyendo Parquet local: /home/work/green_2016_10.parquet\n",
      "📊 [GREEN] Filas totales: 1252572 → Chunks de 500000: 3\n",
      "🧩 [GREEN] Chunk 1/3 → filas [1, 500000] → RUN_ID=GRE_2016_10_0001\n",
      "🧩 [GREEN] Chunk 2/3 → filas [500001, 1000000] → RUN_ID=GRE_2016_10_0002\n",
      "🧩 [GREEN] Chunk 3/3 → filas [1000001, 1252572] → RUN_ID=GRE_2016_10_0003\n",
      "✅ [GREEN] Ingesta mensual chunked COMPLETADA con auditoría por chunk.\n",
      "\n",
      "================================================================================\n",
      "📅 [GREEN] Procesando 2016-11 → /home/work/green_2016_11.parquet\n",
      "📥 [GREEN] Leyendo Parquet local: /home/work/green_2016_11.parquet\n",
      "📊 [GREEN] Filas totales: 1148214 → Chunks de 500000: 3\n",
      "🧩 [GREEN] Chunk 1/3 → filas [1, 500000] → RUN_ID=GRE_2016_11_0001\n",
      "🧩 [GREEN] Chunk 2/3 → filas [500001, 1000000] → RUN_ID=GRE_2016_11_0002\n",
      "🧩 [GREEN] Chunk 3/3 → filas [1000001, 1148214] → RUN_ID=GRE_2016_11_0003\n",
      "✅ [GREEN] Ingesta mensual chunked COMPLETADA con auditoría por chunk.\n",
      "\n",
      "================================================================================\n",
      "📅 [GREEN] Procesando 2016-12 → /home/work/green_2016_12.parquet\n",
      "📥 [GREEN] Leyendo Parquet local: /home/work/green_2016_12.parquet\n",
      "📊 [GREEN] Filas totales: 1224158 → Chunks de 500000: 3\n",
      "🧩 [GREEN] Chunk 1/3 → filas [1, 500000] → RUN_ID=GRE_2016_12_0001\n",
      "🧩 [GREEN] Chunk 2/3 → filas [500001, 1000000] → RUN_ID=GRE_2016_12_0002\n",
      "🧩 [GREEN] Chunk 3/3 → filas [1000001, 1224158] → RUN_ID=GRE_2016_12_0003\n",
      "✅ [GREEN] Ingesta mensual chunked COMPLETADA con auditoría por chunk.\n",
      "\n",
      "================================================================================\n",
      "📅 [GREEN] Procesando 2017-01 → /home/work/green_2017_01.parquet\n",
      "📥 [GREEN] Leyendo Parquet local: /home/work/green_2017_01.parquet\n",
      "📊 [GREEN] Filas totales: 1069565 → Chunks de 500000: 3\n",
      "🧩 [GREEN] Chunk 1/3 → filas [1, 500000] → RUN_ID=GRE_2017_01_0001\n",
      "🧩 [GREEN] Chunk 2/3 → filas [500001, 1000000] → RUN_ID=GRE_2017_01_0002\n",
      "🧩 [GREEN] Chunk 3/3 → filas [1000001, 1069565] → RUN_ID=GRE_2017_01_0003\n",
      "✅ [GREEN] Ingesta mensual chunked COMPLETADA con auditoría por chunk.\n",
      "\n",
      "================================================================================\n",
      "📅 [GREEN] Procesando 2017-02 → /home/work/green_2017_02.parquet\n",
      "📥 [GREEN] Leyendo Parquet local: /home/work/green_2017_02.parquet\n",
      "📊 [GREEN] Filas totales: 1022313 → Chunks de 500000: 3\n",
      "🧩 [GREEN] Chunk 1/3 → filas [1, 500000] → RUN_ID=GRE_2017_02_0001\n",
      "🧩 [GREEN] Chunk 2/3 → filas [500001, 1000000] → RUN_ID=GRE_2017_02_0002\n",
      "🧩 [GREEN] Chunk 3/3 → filas [1000001, 1022313] → RUN_ID=GRE_2017_02_0003\n",
      "✅ [GREEN] Ingesta mensual chunked COMPLETADA con auditoría por chunk.\n",
      "\n",
      "================================================================================\n",
      "📅 [GREEN] Procesando 2017-03 → /home/work/green_2017_03.parquet\n",
      "📥 [GREEN] Leyendo Parquet local: /home/work/green_2017_03.parquet\n",
      "📊 [GREEN] Filas totales: 1157827 → Chunks de 500000: 3\n",
      "🧩 [GREEN] Chunk 1/3 → filas [1, 500000] → RUN_ID=GRE_2017_03_0001\n",
      "🧩 [GREEN] Chunk 2/3 → filas [500001, 1000000] → RUN_ID=GRE_2017_03_0002\n",
      "🧩 [GREEN] Chunk 3/3 → filas [1000001, 1157827] → RUN_ID=GRE_2017_03_0003\n",
      "✅ [GREEN] Ingesta mensual chunked COMPLETADA con auditoría por chunk.\n",
      "\n",
      "================================================================================\n",
      "📅 [GREEN] Procesando 2017-04 → /home/work/green_2017_04.parquet\n",
      "📥 [GREEN] Leyendo Parquet local: /home/work/green_2017_04.parquet\n",
      "📊 [GREEN] Filas totales: 1080844 → Chunks de 500000: 3\n",
      "🧩 [GREEN] Chunk 1/3 → filas [1, 500000] → RUN_ID=GRE_2017_04_0001\n",
      "🧩 [GREEN] Chunk 2/3 → filas [500001, 1000000] → RUN_ID=GRE_2017_04_0002\n",
      "🧩 [GREEN] Chunk 3/3 → filas [1000001, 1080844] → RUN_ID=GRE_2017_04_0003\n",
      "✅ [GREEN] Ingesta mensual chunked COMPLETADA con auditoría por chunk.\n",
      "\n",
      "================================================================================\n",
      "📅 [GREEN] Procesando 2017-05 → /home/work/green_2017_05.parquet\n",
      "📥 [GREEN] Leyendo Parquet local: /home/work/green_2017_05.parquet\n",
      "📊 [GREEN] Filas totales: 1059463 → Chunks de 500000: 3\n",
      "🧩 [GREEN] Chunk 1/3 → filas [1, 500000] → RUN_ID=GRE_2017_05_0001\n",
      "🧩 [GREEN] Chunk 2/3 → filas [500001, 1000000] → RUN_ID=GRE_2017_05_0002\n",
      "🧩 [GREEN] Chunk 3/3 → filas [1000001, 1059463] → RUN_ID=GRE_2017_05_0003\n",
      "✅ [GREEN] Ingesta mensual chunked COMPLETADA con auditoría por chunk.\n",
      "\n",
      "================================================================================\n",
      "📅 [GREEN] Procesando 2017-06 → /home/work/green_2017_06.parquet\n",
      "📥 [GREEN] Leyendo Parquet local: /home/work/green_2017_06.parquet\n",
      "📊 [GREEN] Filas totales: 976467 → Chunks de 500000: 2\n",
      "🧩 [GREEN] Chunk 1/2 → filas [1, 500000] → RUN_ID=GRE_2017_06_0001\n",
      "🧩 [GREEN] Chunk 2/2 → filas [500001, 976467] → RUN_ID=GRE_2017_06_0002\n",
      "✅ [GREEN] Ingesta mensual chunked COMPLETADA con auditoría por chunk.\n",
      "\n",
      "================================================================================\n",
      "📅 [GREEN] Procesando 2017-07 → /home/work/green_2017_07.parquet\n",
      "📥 [GREEN] Leyendo Parquet local: /home/work/green_2017_07.parquet\n",
      "📊 [GREEN] Filas totales: 914783 → Chunks de 500000: 2\n",
      "🧩 [GREEN] Chunk 1/2 → filas [1, 500000] → RUN_ID=GRE_2017_07_0001\n",
      "🧩 [GREEN] Chunk 2/2 → filas [500001, 914783] → RUN_ID=GRE_2017_07_0002\n",
      "✅ [GREEN] Ingesta mensual chunked COMPLETADA con auditoría por chunk.\n",
      "\n",
      "================================================================================\n",
      "📅 [GREEN] Procesando 2017-08 → /home/work/green_2017_08.parquet\n",
      "📥 [GREEN] Leyendo Parquet local: /home/work/green_2017_08.parquet\n",
      "📊 [GREEN] Filas totales: 867407 → Chunks de 500000: 2\n",
      "🧩 [GREEN] Chunk 1/2 → filas [1, 500000] → RUN_ID=GRE_2017_08_0001\n",
      "🧩 [GREEN] Chunk 2/2 → filas [500001, 867407] → RUN_ID=GRE_2017_08_0002\n",
      "✅ [GREEN] Ingesta mensual chunked COMPLETADA con auditoría por chunk.\n",
      "\n",
      "================================================================================\n",
      "📅 [GREEN] Procesando 2017-09 → /home/work/green_2017_09.parquet\n",
      "📥 [GREEN] Leyendo Parquet local: /home/work/green_2017_09.parquet\n",
      "📊 [GREEN] Filas totales: 882464 → Chunks de 500000: 2\n",
      "🧩 [GREEN] Chunk 1/2 → filas [1, 500000] → RUN_ID=GRE_2017_09_0001\n",
      "🧩 [GREEN] Chunk 2/2 → filas [500001, 882464] → RUN_ID=GRE_2017_09_0002\n",
      "✅ [GREEN] Ingesta mensual chunked COMPLETADA con auditoría por chunk.\n",
      "\n",
      "================================================================================\n",
      "📅 [GREEN] Procesando 2017-10 → /home/work/green_2017_10.parquet\n",
      "📥 [GREEN] Leyendo Parquet local: /home/work/green_2017_10.parquet\n",
      "📊 [GREEN] Filas totales: 925737 → Chunks de 500000: 2\n",
      "🧩 [GREEN] Chunk 1/2 → filas [1, 500000] → RUN_ID=GRE_2017_10_0001\n",
      "🧩 [GREEN] Chunk 2/2 → filas [500001, 925737] → RUN_ID=GRE_2017_10_0002\n",
      "✅ [GREEN] Ingesta mensual chunked COMPLETADA con auditoría por chunk.\n",
      "\n",
      "================================================================================\n",
      "📅 [GREEN] Procesando 2017-11 → /home/work/green_2017_11.parquet\n",
      "📥 [GREEN] Leyendo Parquet local: /home/work/green_2017_11.parquet\n",
      "📊 [GREEN] Filas totales: 874173 → Chunks de 500000: 2\n",
      "🧩 [GREEN] Chunk 1/2 → filas [1, 500000] → RUN_ID=GRE_2017_11_0001\n",
      "🧩 [GREEN] Chunk 2/2 → filas [500001, 874173] → RUN_ID=GRE_2017_11_0002\n",
      "✅ [GREEN] Ingesta mensual chunked COMPLETADA con auditoría por chunk.\n",
      "\n",
      "================================================================================\n",
      "📅 [GREEN] Procesando 2017-12 → /home/work/green_2017_12.parquet\n",
      "📥 [GREEN] Leyendo Parquet local: /home/work/green_2017_12.parquet\n",
      "📊 [GREEN] Filas totales: 906016 → Chunks de 500000: 2\n",
      "🧩 [GREEN] Chunk 1/2 → filas [1, 500000] → RUN_ID=GRE_2017_12_0001\n",
      "🧩 [GREEN] Chunk 2/2 → filas [500001, 906016] → RUN_ID=GRE_2017_12_0002\n",
      "✅ [GREEN] Ingesta mensual chunked COMPLETADA con auditoría por chunk.\n",
      "\n",
      "================================================================================\n",
      "📅 [GREEN] Procesando 2018-01 → /home/work/green_2018_01.parquet\n",
      "📥 [GREEN] Leyendo Parquet local: /home/work/green_2018_01.parquet\n",
      "📊 [GREEN] Filas totales: 792744 → Chunks de 500000: 2\n",
      "🧩 [GREEN] Chunk 1/2 → filas [1, 500000] → RUN_ID=GRE_2018_01_0001\n",
      "🧩 [GREEN] Chunk 2/2 → filas [500001, 792744] → RUN_ID=GRE_2018_01_0002\n",
      "✅ [GREEN] Ingesta mensual chunked COMPLETADA con auditoría por chunk.\n",
      "\n",
      "================================================================================\n",
      "📅 [GREEN] Procesando 2018-02 → /home/work/green_2018_02.parquet\n",
      "📥 [GREEN] Leyendo Parquet local: /home/work/green_2018_02.parquet\n",
      "📊 [GREEN] Filas totales: 769197 → Chunks de 500000: 2\n",
      "🧩 [GREEN] Chunk 1/2 → filas [1, 500000] → RUN_ID=GRE_2018_02_0001\n",
      "🧩 [GREEN] Chunk 2/2 → filas [500001, 769197] → RUN_ID=GRE_2018_02_0002\n",
      "✅ [GREEN] Ingesta mensual chunked COMPLETADA con auditoría por chunk.\n",
      "\n",
      "================================================================================\n",
      "📅 [GREEN] Procesando 2018-03 → /home/work/green_2018_03.parquet\n",
      "📥 [GREEN] Leyendo Parquet local: /home/work/green_2018_03.parquet\n",
      "📊 [GREEN] Filas totales: 836246 → Chunks de 500000: 2\n",
      "🧩 [GREEN] Chunk 1/2 → filas [1, 500000] → RUN_ID=GRE_2018_03_0001\n",
      "🧩 [GREEN] Chunk 2/2 → filas [500001, 836246] → RUN_ID=GRE_2018_03_0002\n",
      "✅ [GREEN] Ingesta mensual chunked COMPLETADA con auditoría por chunk.\n",
      "\n",
      "================================================================================\n",
      "📅 [GREEN] Procesando 2018-04 → /home/work/green_2018_04.parquet\n",
      "📥 [GREEN] Leyendo Parquet local: /home/work/green_2018_04.parquet\n",
      "📊 [GREEN] Filas totales: 799383 → Chunks de 500000: 2\n",
      "🧩 [GREEN] Chunk 1/2 → filas [1, 500000] → RUN_ID=GRE_2018_04_0001\n",
      "🧩 [GREEN] Chunk 2/2 → filas [500001, 799383] → RUN_ID=GRE_2018_04_0002\n",
      "✅ [GREEN] Ingesta mensual chunked COMPLETADA con auditoría por chunk.\n",
      "\n",
      "================================================================================\n",
      "📅 [GREEN] Procesando 2018-05 → /home/work/green_2018_05.parquet\n",
      "📥 [GREEN] Leyendo Parquet local: /home/work/green_2018_05.parquet\n",
      "📊 [GREEN] Filas totales: 796552 → Chunks de 500000: 2\n",
      "🧩 [GREEN] Chunk 1/2 → filas [1, 500000] → RUN_ID=GRE_2018_05_0001\n",
      "🧩 [GREEN] Chunk 2/2 → filas [500001, 796552] → RUN_ID=GRE_2018_05_0002\n",
      "✅ [GREEN] Ingesta mensual chunked COMPLETADA con auditoría por chunk.\n",
      "\n",
      "================================================================================\n",
      "📅 [GREEN] Procesando 2018-06 → /home/work/green_2018_06.parquet\n",
      "📥 [GREEN] Leyendo Parquet local: /home/work/green_2018_06.parquet\n",
      "📊 [GREEN] Filas totales: 738546 → Chunks de 500000: 2\n",
      "🧩 [GREEN] Chunk 1/2 → filas [1, 500000] → RUN_ID=GRE_2018_06_0001\n",
      "🧩 [GREEN] Chunk 2/2 → filas [500001, 738546] → RUN_ID=GRE_2018_06_0002\n",
      "✅ [GREEN] Ingesta mensual chunked COMPLETADA con auditoría por chunk.\n",
      "\n",
      "================================================================================\n",
      "📅 [GREEN] Procesando 2018-07 → /home/work/green_2018_07.parquet\n",
      "📥 [GREEN] Leyendo Parquet local: /home/work/green_2018_07.parquet\n",
      "📊 [GREEN] Filas totales: 684374 → Chunks de 500000: 2\n",
      "🧩 [GREEN] Chunk 1/2 → filas [1, 500000] → RUN_ID=GRE_2018_07_0001\n",
      "🧩 [GREEN] Chunk 2/2 → filas [500001, 684374] → RUN_ID=GRE_2018_07_0002\n",
      "✅ [GREEN] Ingesta mensual chunked COMPLETADA con auditoría por chunk.\n",
      "\n",
      "================================================================================\n",
      "📅 [GREEN] Procesando 2018-08 → /home/work/green_2018_08.parquet\n",
      "📥 [GREEN] Leyendo Parquet local: /home/work/green_2018_08.parquet\n",
      "📊 [GREEN] Filas totales: 675815 → Chunks de 500000: 2\n",
      "🧩 [GREEN] Chunk 1/2 → filas [1, 500000] → RUN_ID=GRE_2018_08_0001\n",
      "🧩 [GREEN] Chunk 2/2 → filas [500001, 675815] → RUN_ID=GRE_2018_08_0002\n",
      "✅ [GREEN] Ingesta mensual chunked COMPLETADA con auditoría por chunk.\n",
      "\n",
      "================================================================================\n",
      "📅 [GREEN] Procesando 2018-09 → /home/work/green_2018_09.parquet\n",
      "📥 [GREEN] Leyendo Parquet local: /home/work/green_2018_09.parquet\n",
      "📊 [GREEN] Filas totales: 682032 → Chunks de 500000: 2\n",
      "🧩 [GREEN] Chunk 1/2 → filas [1, 500000] → RUN_ID=GRE_2018_09_0001\n",
      "🧩 [GREEN] Chunk 2/2 → filas [500001, 682032] → RUN_ID=GRE_2018_09_0002\n",
      "✅ [GREEN] Ingesta mensual chunked COMPLETADA con auditoría por chunk.\n",
      "\n",
      "================================================================================\n",
      "📅 [GREEN] Procesando 2018-10 → /home/work/green_2018_10.parquet\n",
      "📥 [GREEN] Leyendo Parquet local: /home/work/green_2018_10.parquet\n",
      "📊 [GREEN] Filas totales: 731888 → Chunks de 500000: 2\n",
      "🧩 [GREEN] Chunk 1/2 → filas [1, 500000] → RUN_ID=GRE_2018_10_0001\n",
      "🧩 [GREEN] Chunk 2/2 → filas [500001, 731888] → RUN_ID=GRE_2018_10_0002\n",
      "✅ [GREEN] Ingesta mensual chunked COMPLETADA con auditoría por chunk.\n",
      "\n",
      "================================================================================\n",
      "📅 [GREEN] Procesando 2018-11 → /home/work/green_2018_11.parquet\n",
      "📥 [GREEN] Leyendo Parquet local: /home/work/green_2018_11.parquet\n",
      "📊 [GREEN] Filas totales: 673287 → Chunks de 500000: 2\n",
      "🧩 [GREEN] Chunk 1/2 → filas [1, 500000] → RUN_ID=GRE_2018_11_0001\n",
      "🧩 [GREEN] Chunk 2/2 → filas [500001, 673287] → RUN_ID=GRE_2018_11_0002\n",
      "✅ [GREEN] Ingesta mensual chunked COMPLETADA con auditoría por chunk.\n",
      "\n",
      "================================================================================\n",
      "📅 [GREEN] Procesando 2018-12 → /home/work/green_2018_12.parquet\n",
      "📥 [GREEN] Leyendo Parquet local: /home/work/green_2018_12.parquet\n",
      "📊 [GREEN] Filas totales: 719654 → Chunks de 500000: 2\n",
      "🧩 [GREEN] Chunk 1/2 → filas [1, 500000] → RUN_ID=GRE_2018_12_0001\n",
      "🧩 [GREEN] Chunk 2/2 → filas [500001, 719654] → RUN_ID=GRE_2018_12_0002\n",
      "✅ [GREEN] Ingesta mensual chunked COMPLETADA con auditoría por chunk.\n",
      "\n",
      "================================================================================\n",
      "📅 [GREEN] Procesando 2019-01 → /home/work/green_2019_01.parquet\n",
      "📥 [GREEN] Leyendo Parquet local: /home/work/green_2019_01.parquet\n",
      "📊 [GREEN] Filas totales: 672105 → Chunks de 500000: 2\n",
      "🧩 [GREEN] Chunk 1/2 → filas [1, 500000] → RUN_ID=GRE_2019_01_0001\n",
      "🧩 [GREEN] Chunk 2/2 → filas [500001, 672105] → RUN_ID=GRE_2019_01_0002\n",
      "✅ [GREEN] Ingesta mensual chunked COMPLETADA con auditoría por chunk.\n",
      "\n",
      "================================================================================\n",
      "📅 [GREEN] Procesando 2019-02 → /home/work/green_2019_02.parquet\n",
      "📥 [GREEN] Leyendo Parquet local: /home/work/green_2019_02.parquet\n",
      "📊 [GREEN] Filas totales: 615594 → Chunks de 500000: 2\n",
      "🧩 [GREEN] Chunk 1/2 → filas [1, 500000] → RUN_ID=GRE_2019_02_0001\n",
      "🧩 [GREEN] Chunk 2/2 → filas [500001, 615594] → RUN_ID=GRE_2019_02_0002\n",
      "✅ [GREEN] Ingesta mensual chunked COMPLETADA con auditoría por chunk.\n",
      "\n",
      "================================================================================\n",
      "📅 [GREEN] Procesando 2019-03 → /home/work/green_2019_03.parquet\n",
      "📥 [GREEN] Leyendo Parquet local: /home/work/green_2019_03.parquet\n",
      "📊 [GREEN] Filas totales: 643063 → Chunks de 500000: 2\n",
      "🧩 [GREEN] Chunk 1/2 → filas [1, 500000] → RUN_ID=GRE_2019_03_0001\n",
      "🧩 [GREEN] Chunk 2/2 → filas [500001, 643063] → RUN_ID=GRE_2019_03_0002\n",
      "✅ [GREEN] Ingesta mensual chunked COMPLETADA con auditoría por chunk.\n",
      "\n",
      "================================================================================\n",
      "📅 [GREEN] Procesando 2019-04 → /home/work/green_2019_04.parquet\n",
      "📥 [GREEN] Leyendo Parquet local: /home/work/green_2019_04.parquet\n",
      "📊 [GREEN] Filas totales: 567852 → Chunks de 500000: 2\n",
      "🧩 [GREEN] Chunk 1/2 → filas [1, 500000] → RUN_ID=GRE_2019_04_0001\n",
      "🧩 [GREEN] Chunk 2/2 → filas [500001, 567852] → RUN_ID=GRE_2019_04_0002\n",
      "✅ [GREEN] Ingesta mensual chunked COMPLETADA con auditoría por chunk.\n",
      "\n",
      "================================================================================\n",
      "📅 [GREEN] Procesando 2019-05 → /home/work/green_2019_05.parquet\n",
      "📥 [GREEN] Leyendo Parquet local: /home/work/green_2019_05.parquet\n",
      "📊 [GREEN] Filas totales: 545452 → Chunks de 500000: 2\n",
      "🧩 [GREEN] Chunk 1/2 → filas [1, 500000] → RUN_ID=GRE_2019_05_0001\n",
      "🧩 [GREEN] Chunk 2/2 → filas [500001, 545452] → RUN_ID=GRE_2019_05_0002\n",
      "✅ [GREEN] Ingesta mensual chunked COMPLETADA con auditoría por chunk.\n",
      "\n",
      "================================================================================\n",
      "📅 [GREEN] Procesando 2019-06 → /home/work/green_2019_06.parquet\n",
      "📥 [GREEN] Leyendo Parquet local: /home/work/green_2019_06.parquet\n",
      "📊 [GREEN] Filas totales: 506238 → Chunks de 500000: 2\n",
      "🧩 [GREEN] Chunk 1/2 → filas [1, 500000] → RUN_ID=GRE_2019_06_0001\n",
      "🧩 [GREEN] Chunk 2/2 → filas [500001, 506238] → RUN_ID=GRE_2019_06_0002\n",
      "✅ [GREEN] Ingesta mensual chunked COMPLETADA con auditoría por chunk.\n",
      "\n",
      "================================================================================\n",
      "📅 [GREEN] Procesando 2019-07 → /home/work/green_2019_07.parquet\n",
      "📥 [GREEN] Leyendo Parquet local: /home/work/green_2019_07.parquet\n",
      "📊 [GREEN] Filas totales: 470743 → Chunks de 500000: 1\n",
      "🧩 [GREEN] Chunk 1/1 → filas [1, 470743] → RUN_ID=GRE_2019_07_0001\n",
      "✅ [GREEN] Ingesta mensual chunked COMPLETADA con auditoría por chunk.\n",
      "\n",
      "================================================================================\n",
      "📅 [GREEN] Procesando 2019-08 → /home/work/green_2019_08.parquet\n",
      "📥 [GREEN] Leyendo Parquet local: /home/work/green_2019_08.parquet\n",
      "📊 [GREEN] Filas totales: 449695 → Chunks de 500000: 1\n",
      "🧩 [GREEN] Chunk 1/1 → filas [1, 449695] → RUN_ID=GRE_2019_08_0001\n",
      "✅ [GREEN] Ingesta mensual chunked COMPLETADA con auditoría por chunk.\n",
      "\n",
      "================================================================================\n",
      "📅 [GREEN] Procesando 2019-09 → /home/work/green_2019_09.parquet\n",
      "📥 [GREEN] Leyendo Parquet local: /home/work/green_2019_09.parquet\n",
      "📊 [GREEN] Filas totales: 449063 → Chunks de 500000: 1\n",
      "🧩 [GREEN] Chunk 1/1 → filas [1, 449063] → RUN_ID=GRE_2019_09_0001\n",
      "✅ [GREEN] Ingesta mensual chunked COMPLETADA con auditoría por chunk.\n",
      "\n",
      "================================================================================\n",
      "📅 [GREEN] Procesando 2019-10 → /home/work/green_2019_10.parquet\n",
      "📥 [GREEN] Leyendo Parquet local: /home/work/green_2019_10.parquet\n",
      "📊 [GREEN] Filas totales: 476386 → Chunks de 500000: 1\n",
      "🧩 [GREEN] Chunk 1/1 → filas [1, 476386] → RUN_ID=GRE_2019_10_0001\n",
      "✅ [GREEN] Ingesta mensual chunked COMPLETADA con auditoría por chunk.\n",
      "\n",
      "================================================================================\n",
      "📅 [GREEN] Procesando 2019-11 → /home/work/green_2019_11.parquet\n",
      "📥 [GREEN] Leyendo Parquet local: /home/work/green_2019_11.parquet\n",
      "📊 [GREEN] Filas totales: 449500 → Chunks de 500000: 1\n",
      "🧩 [GREEN] Chunk 1/1 → filas [1, 449500] → RUN_ID=GRE_2019_11_0001\n",
      "✅ [GREEN] Ingesta mensual chunked COMPLETADA con auditoría por chunk.\n",
      "\n",
      "================================================================================\n",
      "📅 [GREEN] Procesando 2019-12 → /home/work/green_2019_12.parquet\n",
      "📥 [GREEN] Leyendo Parquet local: /home/work/green_2019_12.parquet\n",
      "📊 [GREEN] Filas totales: 455294 → Chunks de 500000: 1\n",
      "🧩 [GREEN] Chunk 1/1 → filas [1, 455294] → RUN_ID=GRE_2019_12_0001\n",
      "✅ [GREEN] Ingesta mensual chunked COMPLETADA con auditoría por chunk.\n",
      "\n",
      "================================================================================\n",
      "📅 [GREEN] Procesando 2020-01 → /home/work/green_2020_01.parquet\n",
      "📥 [GREEN] Leyendo Parquet local: /home/work/green_2020_01.parquet\n",
      "📊 [GREEN] Filas totales: 447770 → Chunks de 500000: 1\n",
      "🧩 [GREEN] Chunk 1/1 → filas [1, 447770] → RUN_ID=GRE_2020_01_0001\n",
      "✅ [GREEN] Ingesta mensual chunked COMPLETADA con auditoría por chunk.\n",
      "\n",
      "================================================================================\n",
      "📅 [GREEN] Procesando 2020-02 → /home/work/green_2020_02.parquet\n",
      "📥 [GREEN] Leyendo Parquet local: /home/work/green_2020_02.parquet\n",
      "📊 [GREEN] Filas totales: 398632 → Chunks de 500000: 1\n",
      "🧩 [GREEN] Chunk 1/1 → filas [1, 398632] → RUN_ID=GRE_2020_02_0001\n",
      "✅ [GREEN] Ingesta mensual chunked COMPLETADA con auditoría por chunk.\n",
      "\n",
      "================================================================================\n",
      "📅 [GREEN] Procesando 2020-03 → /home/work/green_2020_03.parquet\n",
      "📥 [GREEN] Leyendo Parquet local: /home/work/green_2020_03.parquet\n",
      "📊 [GREEN] Filas totales: 223496 → Chunks de 500000: 1\n",
      "🧩 [GREEN] Chunk 1/1 → filas [1, 223496] → RUN_ID=GRE_2020_03_0001\n",
      "✅ [GREEN] Ingesta mensual chunked COMPLETADA con auditoría por chunk.\n",
      "\n",
      "================================================================================\n",
      "📅 [GREEN] Procesando 2020-04 → /home/work/green_2020_04.parquet\n",
      "📥 [GREEN] Leyendo Parquet local: /home/work/green_2020_04.parquet\n",
      "📊 [GREEN] Filas totales: 35644 → Chunks de 500000: 1\n",
      "🧩 [GREEN] Chunk 1/1 → filas [1, 35644] → RUN_ID=GRE_2020_04_0001\n",
      "✅ [GREEN] Ingesta mensual chunked COMPLETADA con auditoría por chunk.\n",
      "\n",
      "================================================================================\n",
      "📅 [GREEN] Procesando 2020-05 → /home/work/green_2020_05.parquet\n",
      "📥 [GREEN] Leyendo Parquet local: /home/work/green_2020_05.parquet\n",
      "📊 [GREEN] Filas totales: 57361 → Chunks de 500000: 1\n",
      "🧩 [GREEN] Chunk 1/1 → filas [1, 57361] → RUN_ID=GRE_2020_05_0001\n",
      "✅ [GREEN] Ingesta mensual chunked COMPLETADA con auditoría por chunk.\n",
      "\n",
      "================================================================================\n",
      "📅 [GREEN] Procesando 2020-06 → /home/work/green_2020_06.parquet\n",
      "📥 [GREEN] Leyendo Parquet local: /home/work/green_2020_06.parquet\n",
      "📊 [GREEN] Filas totales: 63110 → Chunks de 500000: 1\n",
      "🧩 [GREEN] Chunk 1/1 → filas [1, 63110] → RUN_ID=GRE_2020_06_0001\n",
      "✅ [GREEN] Ingesta mensual chunked COMPLETADA con auditoría por chunk.\n",
      "\n",
      "================================================================================\n",
      "📅 [GREEN] Procesando 2020-07 → /home/work/green_2020_07.parquet\n",
      "📥 [GREEN] Leyendo Parquet local: /home/work/green_2020_07.parquet\n",
      "📊 [GREEN] Filas totales: 72258 → Chunks de 500000: 1\n",
      "🧩 [GREEN] Chunk 1/1 → filas [1, 72258] → RUN_ID=GRE_2020_07_0001\n",
      "✅ [GREEN] Ingesta mensual chunked COMPLETADA con auditoría por chunk.\n",
      "\n",
      "================================================================================\n",
      "📅 [GREEN] Procesando 2020-08 → /home/work/green_2020_08.parquet\n",
      "📥 [GREEN] Leyendo Parquet local: /home/work/green_2020_08.parquet\n",
      "📊 [GREEN] Filas totales: 81063 → Chunks de 500000: 1\n",
      "🧩 [GREEN] Chunk 1/1 → filas [1, 81063] → RUN_ID=GRE_2020_08_0001\n",
      "✅ [GREEN] Ingesta mensual chunked COMPLETADA con auditoría por chunk.\n",
      "\n",
      "================================================================================\n",
      "📅 [GREEN] Procesando 2020-09 → /home/work/green_2020_09.parquet\n",
      "📥 [GREEN] Leyendo Parquet local: /home/work/green_2020_09.parquet\n",
      "📊 [GREEN] Filas totales: 87987 → Chunks de 500000: 1\n",
      "🧩 [GREEN] Chunk 1/1 → filas [1, 87987] → RUN_ID=GRE_2020_09_0001\n",
      "✅ [GREEN] Ingesta mensual chunked COMPLETADA con auditoría por chunk.\n",
      "\n",
      "================================================================================\n",
      "📅 [GREEN] Procesando 2020-10 → /home/work/green_2020_10.parquet\n",
      "📥 [GREEN] Leyendo Parquet local: /home/work/green_2020_10.parquet\n",
      "📊 [GREEN] Filas totales: 95120 → Chunks de 500000: 1\n",
      "🧩 [GREEN] Chunk 1/1 → filas [1, 95120] → RUN_ID=GRE_2020_10_0001\n",
      "✅ [GREEN] Ingesta mensual chunked COMPLETADA con auditoría por chunk.\n",
      "\n",
      "================================================================================\n",
      "📅 [GREEN] Procesando 2020-11 → /home/work/green_2020_11.parquet\n",
      "📥 [GREEN] Leyendo Parquet local: /home/work/green_2020_11.parquet\n",
      "📊 [GREEN] Filas totales: 88605 → Chunks de 500000: 1\n",
      "🧩 [GREEN] Chunk 1/1 → filas [1, 88605] → RUN_ID=GRE_2020_11_0001\n",
      "✅ [GREEN] Ingesta mensual chunked COMPLETADA con auditoría por chunk.\n",
      "\n",
      "================================================================================\n",
      "📅 [GREEN] Procesando 2020-12 → /home/work/green_2020_12.parquet\n",
      "📥 [GREEN] Leyendo Parquet local: /home/work/green_2020_12.parquet\n",
      "📊 [GREEN] Filas totales: 83130 → Chunks de 500000: 1\n",
      "🧩 [GREEN] Chunk 1/1 → filas [1, 83130] → RUN_ID=GRE_2020_12_0001\n",
      "✅ [GREEN] Ingesta mensual chunked COMPLETADA con auditoría por chunk.\n",
      "\n",
      "================================================================================\n",
      "📅 [GREEN] Procesando 2021-01 → /home/work/green_2021_01.parquet\n",
      "📥 [GREEN] Leyendo Parquet local: /home/work/green_2021_01.parquet\n",
      "📊 [GREEN] Filas totales: 76518 → Chunks de 500000: 1\n",
      "🧩 [GREEN] Chunk 1/1 → filas [1, 76518] → RUN_ID=GRE_2021_01_0001\n",
      "✅ [GREEN] Ingesta mensual chunked COMPLETADA con auditoría por chunk.\n",
      "\n",
      "================================================================================\n",
      "📅 [GREEN] Procesando 2021-02 → /home/work/green_2021_02.parquet\n",
      "📥 [GREEN] Leyendo Parquet local: /home/work/green_2021_02.parquet\n",
      "📊 [GREEN] Filas totales: 64572 → Chunks de 500000: 1\n",
      "🧩 [GREEN] Chunk 1/1 → filas [1, 64572] → RUN_ID=GRE_2021_02_0001\n",
      "✅ [GREEN] Ingesta mensual chunked COMPLETADA con auditoría por chunk.\n",
      "\n",
      "================================================================================\n",
      "📅 [GREEN] Procesando 2021-03 → /home/work/green_2021_03.parquet\n",
      "📥 [GREEN] Leyendo Parquet local: /home/work/green_2021_03.parquet\n",
      "📊 [GREEN] Filas totales: 83827 → Chunks de 500000: 1\n",
      "🧩 [GREEN] Chunk 1/1 → filas [1, 83827] → RUN_ID=GRE_2021_03_0001\n",
      "✅ [GREEN] Ingesta mensual chunked COMPLETADA con auditoría por chunk.\n",
      "\n",
      "================================================================================\n",
      "📅 [GREEN] Procesando 2021-04 → /home/work/green_2021_04.parquet\n",
      "📥 [GREEN] Leyendo Parquet local: /home/work/green_2021_04.parquet\n",
      "📊 [GREEN] Filas totales: 86941 → Chunks de 500000: 1\n",
      "🧩 [GREEN] Chunk 1/1 → filas [1, 86941] → RUN_ID=GRE_2021_04_0001\n",
      "✅ [GREEN] Ingesta mensual chunked COMPLETADA con auditoría por chunk.\n",
      "\n",
      "================================================================================\n",
      "📅 [GREEN] Procesando 2021-05 → /home/work/green_2021_05.parquet\n",
      "📥 [GREEN] Leyendo Parquet local: /home/work/green_2021_05.parquet\n",
      "📊 [GREEN] Filas totales: 88180 → Chunks de 500000: 1\n",
      "🧩 [GREEN] Chunk 1/1 → filas [1, 88180] → RUN_ID=GRE_2021_05_0001\n",
      "✅ [GREEN] Ingesta mensual chunked COMPLETADA con auditoría por chunk.\n",
      "\n",
      "================================================================================\n",
      "📅 [GREEN] Procesando 2021-06 → /home/work/green_2021_06.parquet\n",
      "📥 [GREEN] Leyendo Parquet local: /home/work/green_2021_06.parquet\n",
      "📊 [GREEN] Filas totales: 86737 → Chunks de 500000: 1\n",
      "🧩 [GREEN] Chunk 1/1 → filas [1, 86737] → RUN_ID=GRE_2021_06_0001\n",
      "✅ [GREEN] Ingesta mensual chunked COMPLETADA con auditoría por chunk.\n",
      "\n",
      "================================================================================\n",
      "📅 [GREEN] Procesando 2021-07 → /home/work/green_2021_07.parquet\n",
      "📥 [GREEN] Leyendo Parquet local: /home/work/green_2021_07.parquet\n",
      "📊 [GREEN] Filas totales: 83691 → Chunks de 500000: 1\n",
      "🧩 [GREEN] Chunk 1/1 → filas [1, 83691] → RUN_ID=GRE_2021_07_0001\n",
      "✅ [GREEN] Ingesta mensual chunked COMPLETADA con auditoría por chunk.\n",
      "\n",
      "================================================================================\n",
      "📅 [GREEN] Procesando 2021-08 → /home/work/green_2021_08.parquet\n",
      "📥 [GREEN] Leyendo Parquet local: /home/work/green_2021_08.parquet\n",
      "📊 [GREEN] Filas totales: 83499 → Chunks de 500000: 1\n",
      "🧩 [GREEN] Chunk 1/1 → filas [1, 83499] → RUN_ID=GRE_2021_08_0001\n",
      "✅ [GREEN] Ingesta mensual chunked COMPLETADA con auditoría por chunk.\n",
      "\n",
      "================================================================================\n",
      "📅 [GREEN] Procesando 2021-09 → /home/work/green_2021_09.parquet\n",
      "📥 [GREEN] Leyendo Parquet local: /home/work/green_2021_09.parquet\n",
      "📊 [GREEN] Filas totales: 95709 → Chunks de 500000: 1\n",
      "🧩 [GREEN] Chunk 1/1 → filas [1, 95709] → RUN_ID=GRE_2021_09_0001\n",
      "✅ [GREEN] Ingesta mensual chunked COMPLETADA con auditoría por chunk.\n",
      "\n",
      "================================================================================\n",
      "📅 [GREEN] Procesando 2021-10 → /home/work/green_2021_10.parquet\n",
      "📥 [GREEN] Leyendo Parquet local: /home/work/green_2021_10.parquet\n",
      "📊 [GREEN] Filas totales: 110891 → Chunks de 500000: 1\n",
      "🧩 [GREEN] Chunk 1/1 → filas [1, 110891] → RUN_ID=GRE_2021_10_0001\n",
      "✅ [GREEN] Ingesta mensual chunked COMPLETADA con auditoría por chunk.\n",
      "\n",
      "================================================================================\n",
      "📅 [GREEN] Procesando 2021-11 → /home/work/green_2021_11.parquet\n",
      "📥 [GREEN] Leyendo Parquet local: /home/work/green_2021_11.parquet\n",
      "📊 [GREEN] Filas totales: 108229 → Chunks de 500000: 1\n",
      "🧩 [GREEN] Chunk 1/1 → filas [1, 108229] → RUN_ID=GRE_2021_11_0001\n",
      "✅ [GREEN] Ingesta mensual chunked COMPLETADA con auditoría por chunk.\n",
      "\n",
      "================================================================================\n",
      "📅 [GREEN] Procesando 2021-12 → /home/work/green_2021_12.parquet\n",
      "📥 [GREEN] Leyendo Parquet local: /home/work/green_2021_12.parquet\n",
      "📊 [GREEN] Filas totales: 99961 → Chunks de 500000: 1\n",
      "🧩 [GREEN] Chunk 1/1 → filas [1, 99961] → RUN_ID=GRE_2021_12_0001\n",
      "✅ [GREEN] Ingesta mensual chunked COMPLETADA con auditoría por chunk.\n",
      "\n",
      "================================================================================\n",
      "📅 [GREEN] Procesando 2022-01 → /home/work/green_2022_01.parquet\n",
      "📥 [GREEN] Leyendo Parquet local: /home/work/green_2022_01.parquet\n",
      "📊 [GREEN] Filas totales: 62495 → Chunks de 500000: 1\n",
      "🧩 [GREEN] Chunk 1/1 → filas [1, 62495] → RUN_ID=GRE_2022_01_0001\n",
      "✅ [GREEN] Ingesta mensual chunked COMPLETADA con auditoría por chunk.\n",
      "\n",
      "================================================================================\n",
      "📅 [GREEN] Procesando 2022-02 → /home/work/green_2022_02.parquet\n",
      "📥 [GREEN] Leyendo Parquet local: /home/work/green_2022_02.parquet\n",
      "📊 [GREEN] Filas totales: 69399 → Chunks de 500000: 1\n",
      "🧩 [GREEN] Chunk 1/1 → filas [1, 69399] → RUN_ID=GRE_2022_02_0001\n",
      "✅ [GREEN] Ingesta mensual chunked COMPLETADA con auditoría por chunk.\n",
      "\n",
      "================================================================================\n",
      "📅 [GREEN] Procesando 2022-03 → /home/work/green_2022_03.parquet\n",
      "📥 [GREEN] Leyendo Parquet local: /home/work/green_2022_03.parquet\n",
      "📊 [GREEN] Filas totales: 78537 → Chunks de 500000: 1\n",
      "🧩 [GREEN] Chunk 1/1 → filas [1, 78537] → RUN_ID=GRE_2022_03_0001\n",
      "✅ [GREEN] Ingesta mensual chunked COMPLETADA con auditoría por chunk.\n",
      "\n",
      "================================================================================\n",
      "📅 [GREEN] Procesando 2022-04 → /home/work/green_2022_04.parquet\n",
      "📥 [GREEN] Leyendo Parquet local: /home/work/green_2022_04.parquet\n",
      "📊 [GREEN] Filas totales: 76136 → Chunks de 500000: 1\n",
      "🧩 [GREEN] Chunk 1/1 → filas [1, 76136] → RUN_ID=GRE_2022_04_0001\n",
      "✅ [GREEN] Ingesta mensual chunked COMPLETADA con auditoría por chunk.\n",
      "\n",
      "================================================================================\n",
      "📅 [GREEN] Procesando 2022-05 → /home/work/green_2022_05.parquet\n",
      "📥 [GREEN] Leyendo Parquet local: /home/work/green_2022_05.parquet\n",
      "📊 [GREEN] Filas totales: 76891 → Chunks de 500000: 1\n",
      "🧩 [GREEN] Chunk 1/1 → filas [1, 76891] → RUN_ID=GRE_2022_05_0001\n",
      "✅ [GREEN] Ingesta mensual chunked COMPLETADA con auditoría por chunk.\n",
      "\n",
      "================================================================================\n",
      "📅 [GREEN] Procesando 2022-06 → /home/work/green_2022_06.parquet\n",
      "📥 [GREEN] Leyendo Parquet local: /home/work/green_2022_06.parquet\n",
      "📊 [GREEN] Filas totales: 73718 → Chunks de 500000: 1\n",
      "🧩 [GREEN] Chunk 1/1 → filas [1, 73718] → RUN_ID=GRE_2022_06_0001\n",
      "✅ [GREEN] Ingesta mensual chunked COMPLETADA con auditoría por chunk.\n",
      "\n",
      "================================================================================\n",
      "📅 [GREEN] Procesando 2022-07 → /home/work/green_2022_07.parquet\n",
      "📥 [GREEN] Leyendo Parquet local: /home/work/green_2022_07.parquet\n",
      "📊 [GREEN] Filas totales: 64192 → Chunks de 500000: 1\n",
      "🧩 [GREEN] Chunk 1/1 → filas [1, 64192] → RUN_ID=GRE_2022_07_0001\n",
      "✅ [GREEN] Ingesta mensual chunked COMPLETADA con auditoría por chunk.\n",
      "\n",
      "================================================================================\n",
      "📅 [GREEN] Procesando 2022-08 → /home/work/green_2022_08.parquet\n",
      "📥 [GREEN] Leyendo Parquet local: /home/work/green_2022_08.parquet\n",
      "📊 [GREEN] Filas totales: 65929 → Chunks de 500000: 1\n",
      "🧩 [GREEN] Chunk 1/1 → filas [1, 65929] → RUN_ID=GRE_2022_08_0001\n",
      "✅ [GREEN] Ingesta mensual chunked COMPLETADA con auditoría por chunk.\n",
      "\n",
      "================================================================================\n",
      "📅 [GREEN] Procesando 2022-09 → /home/work/green_2022_09.parquet\n",
      "📥 [GREEN] Leyendo Parquet local: /home/work/green_2022_09.parquet\n",
      "📊 [GREEN] Filas totales: 69031 → Chunks de 500000: 1\n",
      "🧩 [GREEN] Chunk 1/1 → filas [1, 69031] → RUN_ID=GRE_2022_09_0001\n",
      "✅ [GREEN] Ingesta mensual chunked COMPLETADA con auditoría por chunk.\n",
      "\n",
      "================================================================================\n",
      "📅 [GREEN] Procesando 2022-10 → /home/work/green_2022_10.parquet\n",
      "📥 [GREEN] Leyendo Parquet local: /home/work/green_2022_10.parquet\n",
      "📊 [GREEN] Filas totales: 69322 → Chunks de 500000: 1\n",
      "🧩 [GREEN] Chunk 1/1 → filas [1, 69322] → RUN_ID=GRE_2022_10_0001\n",
      "✅ [GREEN] Ingesta mensual chunked COMPLETADA con auditoría por chunk.\n",
      "\n",
      "================================================================================\n",
      "📅 [GREEN] Procesando 2022-11 → /home/work/green_2022_11.parquet\n",
      "📥 [GREEN] Leyendo Parquet local: /home/work/green_2022_11.parquet\n",
      "📊 [GREEN] Filas totales: 62313 → Chunks de 500000: 1\n",
      "🧩 [GREEN] Chunk 1/1 → filas [1, 62313] → RUN_ID=GRE_2022_11_0001\n",
      "✅ [GREEN] Ingesta mensual chunked COMPLETADA con auditoría por chunk.\n",
      "\n",
      "================================================================================\n",
      "📅 [GREEN] Procesando 2022-12 → /home/work/green_2022_12.parquet\n",
      "📥 [GREEN] Leyendo Parquet local: /home/work/green_2022_12.parquet\n",
      "📊 [GREEN] Filas totales: 72439 → Chunks de 500000: 1\n",
      "🧩 [GREEN] Chunk 1/1 → filas [1, 72439] → RUN_ID=GRE_2022_12_0001\n",
      "✅ [GREEN] Ingesta mensual chunked COMPLETADA con auditoría por chunk.\n",
      "\n",
      "================================================================================\n",
      "📅 [GREEN] Procesando 2023-01 → /home/work/green_2023_01.parquet\n",
      "📥 [GREEN] Leyendo Parquet local: /home/work/green_2023_01.parquet\n",
      "📊 [GREEN] Filas totales: 68211 → Chunks de 500000: 1\n",
      "🧩 [GREEN] Chunk 1/1 → filas [1, 68211] → RUN_ID=GRE_2023_01_0001\n",
      "✅ [GREEN] Ingesta mensual chunked COMPLETADA con auditoría por chunk.\n",
      "\n",
      "================================================================================\n",
      "📅 [GREEN] Procesando 2023-02 → /home/work/green_2023_02.parquet\n",
      "📥 [GREEN] Leyendo Parquet local: /home/work/green_2023_02.parquet\n",
      "📊 [GREEN] Filas totales: 64809 → Chunks de 500000: 1\n",
      "🧩 [GREEN] Chunk 1/1 → filas [1, 64809] → RUN_ID=GRE_2023_02_0001\n",
      "✅ [GREEN] Ingesta mensual chunked COMPLETADA con auditoría por chunk.\n",
      "\n",
      "================================================================================\n",
      "📅 [GREEN] Procesando 2023-03 → /home/work/green_2023_03.parquet\n",
      "📥 [GREEN] Leyendo Parquet local: /home/work/green_2023_03.parquet\n",
      "📊 [GREEN] Filas totales: 72044 → Chunks de 500000: 1\n",
      "🧩 [GREEN] Chunk 1/1 → filas [1, 72044] → RUN_ID=GRE_2023_03_0001\n",
      "✅ [GREEN] Ingesta mensual chunked COMPLETADA con auditoría por chunk.\n",
      "\n",
      "================================================================================\n",
      "📅 [GREEN] Procesando 2023-04 → /home/work/green_2023_04.parquet\n",
      "📥 [GREEN] Leyendo Parquet local: /home/work/green_2023_04.parquet\n",
      "📊 [GREEN] Filas totales: 65392 → Chunks de 500000: 1\n",
      "🧩 [GREEN] Chunk 1/1 → filas [1, 65392] → RUN_ID=GRE_2023_04_0001\n",
      "✅ [GREEN] Ingesta mensual chunked COMPLETADA con auditoría por chunk.\n",
      "\n",
      "================================================================================\n",
      "📅 [GREEN] Procesando 2023-05 → /home/work/green_2023_05.parquet\n",
      "📥 [GREEN] Leyendo Parquet local: /home/work/green_2023_05.parquet\n",
      "📊 [GREEN] Filas totales: 69174 → Chunks de 500000: 1\n",
      "🧩 [GREEN] Chunk 1/1 → filas [1, 69174] → RUN_ID=GRE_2023_05_0001\n",
      "✅ [GREEN] Ingesta mensual chunked COMPLETADA con auditoría por chunk.\n",
      "\n",
      "================================================================================\n",
      "📅 [GREEN] Procesando 2023-06 → /home/work/green_2023_06.parquet\n",
      "📥 [GREEN] Leyendo Parquet local: /home/work/green_2023_06.parquet\n",
      "📊 [GREEN] Filas totales: 65550 → Chunks de 500000: 1\n",
      "🧩 [GREEN] Chunk 1/1 → filas [1, 65550] → RUN_ID=GRE_2023_06_0001\n",
      "✅ [GREEN] Ingesta mensual chunked COMPLETADA con auditoría por chunk.\n",
      "\n",
      "================================================================================\n",
      "📅 [GREEN] Procesando 2023-07 → /home/work/green_2023_07.parquet\n",
      "📥 [GREEN] Leyendo Parquet local: /home/work/green_2023_07.parquet\n",
      "📊 [GREEN] Filas totales: 61343 → Chunks de 500000: 1\n",
      "🧩 [GREEN] Chunk 1/1 → filas [1, 61343] → RUN_ID=GRE_2023_07_0001\n",
      "✅ [GREEN] Ingesta mensual chunked COMPLETADA con auditoría por chunk.\n",
      "\n",
      "================================================================================\n",
      "📅 [GREEN] Procesando 2023-08 → /home/work/green_2023_08.parquet\n",
      "📥 [GREEN] Leyendo Parquet local: /home/work/green_2023_08.parquet\n",
      "📊 [GREEN] Filas totales: 60649 → Chunks de 500000: 1\n",
      "🧩 [GREEN] Chunk 1/1 → filas [1, 60649] → RUN_ID=GRE_2023_08_0001\n",
      "✅ [GREEN] Ingesta mensual chunked COMPLETADA con auditoría por chunk.\n",
      "\n",
      "================================================================================\n",
      "📅 [GREEN] Procesando 2023-09 → /home/work/green_2023_09.parquet\n",
      "📥 [GREEN] Leyendo Parquet local: /home/work/green_2023_09.parquet\n",
      "📊 [GREEN] Filas totales: 65471 → Chunks de 500000: 1\n",
      "🧩 [GREEN] Chunk 1/1 → filas [1, 65471] → RUN_ID=GRE_2023_09_0001\n",
      "✅ [GREEN] Ingesta mensual chunked COMPLETADA con auditoría por chunk.\n",
      "\n",
      "================================================================================\n",
      "📅 [GREEN] Procesando 2023-10 → /home/work/green_2023_10.parquet\n",
      "📥 [GREEN] Leyendo Parquet local: /home/work/green_2023_10.parquet\n",
      "📊 [GREEN] Filas totales: 66177 → Chunks de 500000: 1\n",
      "🧩 [GREEN] Chunk 1/1 → filas [1, 66177] → RUN_ID=GRE_2023_10_0001\n",
      "✅ [GREEN] Ingesta mensual chunked COMPLETADA con auditoría por chunk.\n",
      "\n",
      "================================================================================\n",
      "📅 [GREEN] Procesando 2023-11 → /home/work/green_2023_11.parquet\n",
      "📥 [GREEN] Leyendo Parquet local: /home/work/green_2023_11.parquet\n",
      "📊 [GREEN] Filas totales: 64025 → Chunks de 500000: 1\n",
      "🧩 [GREEN] Chunk 1/1 → filas [1, 64025] → RUN_ID=GRE_2023_11_0001\n",
      "✅ [GREEN] Ingesta mensual chunked COMPLETADA con auditoría por chunk.\n",
      "\n",
      "================================================================================\n",
      "📅 [GREEN] Procesando 2023-12 → /home/work/green_2023_12.parquet\n",
      "📥 [GREEN] Leyendo Parquet local: /home/work/green_2023_12.parquet\n",
      "📊 [GREEN] Filas totales: 64215 → Chunks de 500000: 1\n",
      "🧩 [GREEN] Chunk 1/1 → filas [1, 64215] → RUN_ID=GRE_2023_12_0001\n",
      "✅ [GREEN] Ingesta mensual chunked COMPLETADA con auditoría por chunk.\n",
      "\n",
      "================================================================================\n",
      "📅 [GREEN] Procesando 2024-01 → /home/work/green_2024_01.parquet\n",
      "📥 [GREEN] Leyendo Parquet local: /home/work/green_2024_01.parquet\n",
      "📊 [GREEN] Filas totales: 56551 → Chunks de 500000: 1\n",
      "🧩 [GREEN] Chunk 1/1 → filas [1, 56551] → RUN_ID=GRE_2024_01_0001\n",
      "✅ [GREEN] Ingesta mensual chunked COMPLETADA con auditoría por chunk.\n",
      "\n",
      "================================================================================\n",
      "📅 [GREEN] Procesando 2024-02 → /home/work/green_2024_02.parquet\n",
      "📥 [GREEN] Leyendo Parquet local: /home/work/green_2024_02.parquet\n",
      "📊 [GREEN] Filas totales: 53577 → Chunks de 500000: 1\n",
      "🧩 [GREEN] Chunk 1/1 → filas [1, 53577] → RUN_ID=GRE_2024_02_0001\n",
      "✅ [GREEN] Ingesta mensual chunked COMPLETADA con auditoría por chunk.\n",
      "\n",
      "================================================================================\n",
      "📅 [GREEN] Procesando 2024-03 → /home/work/green_2024_03.parquet\n",
      "📥 [GREEN] Leyendo Parquet local: /home/work/green_2024_03.parquet\n",
      "📊 [GREEN] Filas totales: 57457 → Chunks de 500000: 1\n",
      "🧩 [GREEN] Chunk 1/1 → filas [1, 57457] → RUN_ID=GRE_2024_03_0001\n",
      "✅ [GREEN] Ingesta mensual chunked COMPLETADA con auditoría por chunk.\n",
      "\n",
      "================================================================================\n",
      "📅 [GREEN] Procesando 2024-04 → /home/work/green_2024_04.parquet\n",
      "📥 [GREEN] Leyendo Parquet local: /home/work/green_2024_04.parquet\n",
      "📊 [GREEN] Filas totales: 56471 → Chunks de 500000: 1\n",
      "🧩 [GREEN] Chunk 1/1 → filas [1, 56471] → RUN_ID=GRE_2024_04_0001\n",
      "✅ [GREEN] Ingesta mensual chunked COMPLETADA con auditoría por chunk.\n",
      "\n",
      "================================================================================\n",
      "📅 [GREEN] Procesando 2024-05 → /home/work/green_2024_05.parquet\n",
      "📥 [GREEN] Leyendo Parquet local: /home/work/green_2024_05.parquet\n",
      "📊 [GREEN] Filas totales: 61003 → Chunks de 500000: 1\n",
      "🧩 [GREEN] Chunk 1/1 → filas [1, 61003] → RUN_ID=GRE_2024_05_0001\n",
      "✅ [GREEN] Ingesta mensual chunked COMPLETADA con auditoría por chunk.\n",
      "\n",
      "================================================================================\n",
      "📅 [GREEN] Procesando 2024-06 → /home/work/green_2024_06.parquet\n",
      "📥 [GREEN] Leyendo Parquet local: /home/work/green_2024_06.parquet\n",
      "📊 [GREEN] Filas totales: 54748 → Chunks de 500000: 1\n",
      "🧩 [GREEN] Chunk 1/1 → filas [1, 54748] → RUN_ID=GRE_2024_06_0001\n",
      "✅ [GREEN] Ingesta mensual chunked COMPLETADA con auditoría por chunk.\n",
      "\n",
      "================================================================================\n",
      "📅 [GREEN] Procesando 2024-07 → /home/work/green_2024_07.parquet\n",
      "📥 [GREEN] Leyendo Parquet local: /home/work/green_2024_07.parquet\n",
      "📊 [GREEN] Filas totales: 51837 → Chunks de 500000: 1\n",
      "🧩 [GREEN] Chunk 1/1 → filas [1, 51837] → RUN_ID=GRE_2024_07_0001\n",
      "✅ [GREEN] Ingesta mensual chunked COMPLETADA con auditoría por chunk.\n",
      "\n",
      "================================================================================\n",
      "📅 [GREEN] Procesando 2024-08 → /home/work/green_2024_08.parquet\n",
      "📥 [GREEN] Leyendo Parquet local: /home/work/green_2024_08.parquet\n",
      "📊 [GREEN] Filas totales: 51771 → Chunks de 500000: 1\n",
      "🧩 [GREEN] Chunk 1/1 → filas [1, 51771] → RUN_ID=GRE_2024_08_0001\n",
      "✅ [GREEN] Ingesta mensual chunked COMPLETADA con auditoría por chunk.\n",
      "\n",
      "================================================================================\n",
      "📅 [GREEN] Procesando 2024-09 → /home/work/green_2024_09.parquet\n",
      "📥 [GREEN] Leyendo Parquet local: /home/work/green_2024_09.parquet\n",
      "📊 [GREEN] Filas totales: 54440 → Chunks de 500000: 1\n",
      "🧩 [GREEN] Chunk 1/1 → filas [1, 54440] → RUN_ID=GRE_2024_09_0001\n",
      "✅ [GREEN] Ingesta mensual chunked COMPLETADA con auditoría por chunk.\n",
      "\n",
      "================================================================================\n",
      "📅 [GREEN] Procesando 2024-10 → /home/work/green_2024_10.parquet\n",
      "📥 [GREEN] Leyendo Parquet local: /home/work/green_2024_10.parquet\n",
      "📊 [GREEN] Filas totales: 56147 → Chunks de 500000: 1\n",
      "🧩 [GREEN] Chunk 1/1 → filas [1, 56147] → RUN_ID=GRE_2024_10_0001\n",
      "✅ [GREEN] Ingesta mensual chunked COMPLETADA con auditoría por chunk.\n",
      "\n",
      "================================================================================\n",
      "📅 [GREEN] Procesando 2024-11 → /home/work/green_2024_11.parquet\n",
      "📥 [GREEN] Leyendo Parquet local: /home/work/green_2024_11.parquet\n",
      "📊 [GREEN] Filas totales: 52222 → Chunks de 500000: 1\n",
      "🧩 [GREEN] Chunk 1/1 → filas [1, 52222] → RUN_ID=GRE_2024_11_0001\n",
      "✅ [GREEN] Ingesta mensual chunked COMPLETADA con auditoría por chunk.\n",
      "\n",
      "================================================================================\n",
      "📅 [GREEN] Procesando 2024-12 → /home/work/green_2024_12.parquet\n",
      "📥 [GREEN] Leyendo Parquet local: /home/work/green_2024_12.parquet\n",
      "📊 [GREEN] Filas totales: 53994 → Chunks de 500000: 1\n",
      "🧩 [GREEN] Chunk 1/1 → filas [1, 53994] → RUN_ID=GRE_2024_12_0001\n",
      "✅ [GREEN] Ingesta mensual chunked COMPLETADA con auditoría por chunk.\n",
      "\n",
      "================================================================================\n",
      "📅 [GREEN] Procesando 2025-01 → /home/work/green_2025_01.parquet\n",
      "📥 [GREEN] Leyendo Parquet local: /home/work/green_2025_01.parquet\n",
      "📊 [GREEN] Filas totales: 48326 → Chunks de 500000: 1\n",
      "🧩 [GREEN] Chunk 1/1 → filas [1, 48326] → RUN_ID=GRE_2025_01_0001\n",
      "✅ [GREEN] Ingesta mensual chunked COMPLETADA con auditoría por chunk.\n",
      "\n",
      "================================================================================\n",
      "📅 [GREEN] Procesando 2025-02 → /home/work/green_2025_02.parquet\n",
      "📥 [GREEN] Leyendo Parquet local: /home/work/green_2025_02.parquet\n",
      "📊 [GREEN] Filas totales: 46621 → Chunks de 500000: 1\n",
      "🧩 [GREEN] Chunk 1/1 → filas [1, 46621] → RUN_ID=GRE_2025_02_0001\n",
      "✅ [GREEN] Ingesta mensual chunked COMPLETADA con auditoría por chunk.\n",
      "\n",
      "================================================================================\n",
      "📅 [GREEN] Procesando 2025-03 → /home/work/green_2025_03.parquet\n",
      "📥 [GREEN] Leyendo Parquet local: /home/work/green_2025_03.parquet\n",
      "📊 [GREEN] Filas totales: 51539 → Chunks de 500000: 1\n",
      "🧩 [GREEN] Chunk 1/1 → filas [1, 51539] → RUN_ID=GRE_2025_03_0001\n",
      "✅ [GREEN] Ingesta mensual chunked COMPLETADA con auditoría por chunk.\n",
      "\n",
      "================================================================================\n",
      "📅 [GREEN] Procesando 2025-04 → /home/work/green_2025_04.parquet\n",
      "📥 [GREEN] Leyendo Parquet local: /home/work/green_2025_04.parquet\n",
      "📊 [GREEN] Filas totales: 52132 → Chunks de 500000: 1\n",
      "🧩 [GREEN] Chunk 1/1 → filas [1, 52132] → RUN_ID=GRE_2025_04_0001\n",
      "✅ [GREEN] Ingesta mensual chunked COMPLETADA con auditoría por chunk.\n",
      "\n",
      "================================================================================\n",
      "📅 [GREEN] Procesando 2025-05 → /home/work/green_2025_05.parquet\n",
      "📥 [GREEN] Leyendo Parquet local: /home/work/green_2025_05.parquet\n",
      "📊 [GREEN] Filas totales: 55399 → Chunks de 500000: 1\n",
      "🧩 [GREEN] Chunk 1/1 → filas [1, 55399] → RUN_ID=GRE_2025_05_0001\n",
      "✅ [GREEN] Ingesta mensual chunked COMPLETADA con auditoría por chunk.\n",
      "\n",
      "================================================================================\n",
      "📅 [GREEN] Procesando 2025-06 → /home/work/green_2025_06.parquet\n",
      "📥 [GREEN] Leyendo Parquet local: /home/work/green_2025_06.parquet\n",
      "📊 [GREEN] Filas totales: 49390 → Chunks de 500000: 1\n",
      "🧩 [GREEN] Chunk 1/1 → filas [1, 49390] → RUN_ID=GRE_2025_06_0001\n",
      "✅ [GREEN] Ingesta mensual chunked COMPLETADA con auditoría por chunk.\n",
      "\n",
      "================================================================================\n",
      "📅 [GREEN] Procesando 2025-07 → /home/work/green_2025_07.parquet\n",
      "📥 [GREEN] Leyendo Parquet local: /home/work/green_2025_07.parquet\n",
      "📊 [GREEN] Filas totales: 48205 → Chunks de 500000: 1\n",
      "🧩 [GREEN] Chunk 1/1 → filas [1, 48205] → RUN_ID=GRE_2025_07_0001\n",
      "✅ [GREEN] Ingesta mensual chunked COMPLETADA con auditoría por chunk.\n",
      "\n",
      "================================================================================\n",
      "📅 [GREEN] Procesando 2025-08 → /home/work/green_2025_08.parquet\n",
      "📥 [GREEN] Leyendo Parquet local: /home/work/green_2025_08.parquet\n",
      "📊 [GREEN] Filas totales: 46306 → Chunks de 500000: 1\n",
      "🧩 [GREEN] Chunk 1/1 → filas [1, 46306] → RUN_ID=GRE_2025_08_0001\n",
      "✅ [GREEN] Ingesta mensual chunked COMPLETADA con auditoría por chunk.\n",
      "\n",
      "================================================================================\n",
      "🏁 [GREEN] Resumen:\n",
      "   ✅ Meses OK     : 115\n",
      "   ⏭  Meses omitidos (sin archivo): 0\n",
      "   ❌ Meses con error             : 0\n",
      "🎉 [GREEN] Terminado.\n"
     ]
    }
   ],
   "source": [
    "# -----------------------\n",
    "# Runner multi-mes para GREEN (ajusta rango/base_dir según tus archivos locales)\n",
    "# -----------------------\n",
    "from datetime import date\n",
    "import os\n",
    "\n",
    "# Usa tu month_iter existente; si no, descomenta:\n",
    "def month_iter(start_year: int, start_month: int, end_year: int, end_month: int):\n",
    " y, m = start_year, start_month\n",
    " while (y < end_year) or (y == end_year and m <= end_month):\n",
    "     yield y, m\n",
    "     m = 1 if m == 12 else m + 1\n",
    "     y = y + 1 if m == 1 else y\n",
    "\n",
    "START_YEAR_G, START_MONTH_G = 2016,2\n",
    "END_YEAR_G,   END_MONTH_G   = 2025, 8\n",
    "\n",
    "base_dir_green = \"/home/work\"  # carpeta donde están los parquet locales\n",
    "ok_g, skipped_g, failed_g = 0, 0, 0\n",
    "\n",
    "print(f\"🚀 [GREEN] Iniciando ingesta mensual: {START_YEAR_G}-{START_MONTH_G:02d} → {END_YEAR_G}-{END_MONTH_G:02d}\")\n",
    "for yy, mm in month_iter(START_YEAR_G, START_MONTH_G, END_YEAR_G, END_MONTH_G):\n",
    "    file_path = os.path.join(base_dir_green, f\"green_{yy}_{mm:02d}.parquet\")  # p.ej., green_2017_08.parquet\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"📅 [GREEN] Procesando {yy}-{mm:02d} → {file_path}\")\n",
    "\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"⏭  [GREEN] Archivo NO encontrado, se omite: {file_path}\")\n",
    "        skipped_g += 1\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        ingest_green_month_chunked(\n",
    "            spark=spark,\n",
    "            sfOptions=sfOptions,\n",
    "            file_path=file_path,\n",
    "            year=yy,\n",
    "            month=mm,\n",
    "            chunk_size=CHUNK_SIZE_GREEN\n",
    "        )\n",
    "        ok_g += 1\n",
    "    except Exception as e:\n",
    "        failed_g += 1\n",
    "        print(f\"❌ [GREEN] Error en {yy}-{mm:02d}: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"🏁 [GREEN] Resumen:\")\n",
    "print(f\"   ✅ Meses OK     : {ok_g}\")\n",
    "print(f\"   ⏭  Meses omitidos (sin archivo): {skipped_g}\")\n",
    "print(f\"   ❌ Meses con error             : {failed_g}\")\n",
    "print(\"🎉 [GREEN] Terminado.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4007286b-452c-4b36-8ef8-62850ebc851a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
